#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
script_02_apply_combine.py (v1.0 - æª”æ¡ˆåˆä½µç‰ˆ)

åŠŸèƒ½ï¼š
1. é¸æ“‡è¦åˆä½µçš„ tobemodified Excel æª”æ¡ˆ
2. é¸æ“‡ i18n_combine ç›®éŒ„ä¸‹çš„ JSON/PO æª”æ¡ˆä½œç‚ºåˆä½µç›®æ¨™
3. æª¢æ¸¬é‡è¤‡ key ä¸¦è™•ç†è¡çª
4. ç”Ÿæˆåˆä½µå¾Œçš„æª”æ¡ˆåˆ° i18n_output/{language}_{timestamp}_combined/
5. æä¾›è©³ç´°çš„åˆä½µå ±å‘Šå’Œæ—¥èªŒ

ä¾æ“šç”¨æˆ¶é¸æ“‡çš„ tobemodified_{language}.xlsxï¼Œå°‡ä¿®æ­£çµæœåˆä½µåˆ°æŒ‡å®šçš„ç¿»è­¯æª”æ¡ˆä¸­
"""

import json
import sys
import shutil
import datetime
import argparse
import glob
from pathlib import Path
from collections import defaultdict
from config_loader import get_config

try:
    import openpyxl
    import polib
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦å¥—ä»¶ï¼š{e}")
    print("è«‹åŸ·è¡Œï¼špip install openpyxl polib")
    sys.exit(1)


def detect_tobemodified_files(config) -> dict:
    """æª¢æ¸¬å¯ç”¨çš„ tobemodified æª”æ¡ˆ"""
    available_files = {}
    
    # æª¢æ¸¬è¼¸å‡ºç›®éŒ„ä¸­çš„æª”æ¡ˆ
    try:
        dirs = config.get_directories()
        output_dir = Path(dirs['output_dir'])
    except Exception:
        output_dir = Path('i18n_output')
    
    # ä½¿ç”¨é…ç½®è¼‰å…¥å™¨çš„èªè¨€æª¢æ¸¬
    try:
        available_languages = config.detect_available_languages()
    except Exception as e:
        print(f"âš ï¸  èªè¨€æª¢æ¸¬å¤±æ•—ï¼š{e}")
        available_languages = []
    
    # æª¢æ¸¬æ¨™æº–å‘½åçš„æª”æ¡ˆ
    for language in available_languages:
        tobemodified_path = output_dir / f"{language}_tobemodified.xlsx"
        if tobemodified_path.exists():
            available_files[language] = tobemodified_path
    
    # åœ¨ç•¶å‰ç›®éŒ„ä¸­æŸ¥æ‰¾é¡å¤–çš„æª”æ¡ˆ
    for file_path in Path('.').glob("*_tobemodified.xlsx"):
        filename = file_path.stem
        if filename.endswith('_tobemodified'):
            language = filename[:-len('_tobemodified')]
            
            # éæ¿¾ç³»çµ±è‡¨æ™‚æª”æ¡ˆ
            if language.startswith(('~$', '.', '__')):
                continue
            
            if language not in available_files:
                available_files[language] = file_path

    return available_files


def scan_combine_directory(combine_dir: Path) -> dict:
    """æƒæ i18n_combine ç›®éŒ„ä¸­çš„æª”æ¡ˆ"""
    files = {
        'json': [],
        'po': []
    }
    
    if not combine_dir.exists():
        return files
    
    # éæ­¸æƒææ‰€æœ‰ JSON å’Œ PO æª”æ¡ˆ
    for file_path in combine_dir.rglob("*.json"):
        relative_path = file_path.relative_to(combine_dir)
        files['json'].append({
            'path': file_path,
            'relative_path': str(relative_path),
            'name': file_path.name
        })
    
    for file_path in combine_dir.rglob("*.po"):
        relative_path = file_path.relative_to(combine_dir)
        files['po'].append({
            'path': file_path,
            'relative_path': str(relative_path),
            'name': file_path.name
        })
    
    return files


def choose_tobemodified_file(available_files: dict) -> tuple:
    """é¸æ“‡è¦ä½¿ç”¨çš„ tobemodified æª”æ¡ˆ"""
    if not available_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• tobemodified æª”æ¡ˆ")
        return None, None
    
    if len(available_files) == 1:
        language, file_path = list(available_files.items())[0]
        print(f"ğŸ¯ è‡ªå‹•é¸æ“‡å”¯ä¸€çš„ tobemodified æª”æ¡ˆï¼š{language} ({file_path.name})")
        return language, file_path
    
    # å¤šå€‹æª”æ¡ˆï¼Œè®“ç”¨æˆ¶é¸æ“‡
    print("\nğŸ“„ å¯ç”¨çš„ tobemodified æª”æ¡ˆï¼š")
    choices = list(available_files.items())
    
    for i, (language, file_path) in enumerate(choices, 1):
        print(f"  {i}) {language} ({file_path.name})")
    
    while True:
        try:
            choice = input(f"\nè«‹é¸æ“‡è¦ä½¿ç”¨çš„æª”æ¡ˆ (1-{len(choices)})ï¼š").strip()
            choice_idx = int(choice) - 1
            
            if 0 <= choice_idx < len(choices):
                language, file_path = choices[choice_idx]
                print(f"âœ… é¸æ“‡äº†ï¼š{language} ({file_path.name})")
                return language, file_path
            else:
                print(f"âš ï¸  è«‹è¼¸å…¥ 1-{len(choices)} ä¹‹é–“çš„æ•¸å­—")
        except (ValueError, KeyboardInterrupt):
            print("\nâŒ æ“ä½œå–æ¶ˆ")
            return None, None


def choose_combine_file(files: list, file_type: str) -> Path:
    """é¸æ“‡è¦åˆä½µçš„æª”æ¡ˆ"""
    if not files:
        print(f"âš ï¸  /i18n_combine/ ä¸­æ²’æœ‰æ‰¾åˆ° {file_type.upper()} æª”æ¡ˆ")
        return None
    
    print(f"\nğŸ“ å¯ç”¨çš„ {file_type.upper()} æª”æ¡ˆï¼š")
    for i, file_info in enumerate(files, 1):
        print(f"  {i}) {file_info['relative_path']}")
    
    print(f"  0) è·³é {file_type.upper()} æª”æ¡ˆ")
    
    while True:
        try:
            choice = input(f"\nè«‹é¸æ“‡è¦åˆä½µçš„ {file_type.upper()} æª”æ¡ˆ (0-{len(files)})ï¼š").strip()
            choice_idx = int(choice)
            
            if choice_idx == 0:
                print(f"â­ï¸  è·³é {file_type.upper()} æª”æ¡ˆ")
                return None
            elif 1 <= choice_idx <= len(files):
                selected_file = files[choice_idx - 1]
                print(f"âœ… é¸æ“‡äº†ï¼š{selected_file['relative_path']}")
                return selected_file['path']
            else:
                print(f"âš ï¸  è«‹è¼¸å…¥ 0-{len(files)} ä¹‹é–“çš„æ•¸å­—")
        except (ValueError, KeyboardInterrupt):
            print("\nâŒ æ“ä½œå–æ¶ˆ")
            return None


def read_excel_updates(xlsx_path: Path, config) -> dict:
    """è®€å– Excel æª”æ¡ˆä¸­çš„æ›´æ–°è³‡æ–™ - è‡ªå‹•è™•ç†æ‰€æœ‰æœ‰æ›¿æ›çµæœçš„æ¥­æ…‹"""
    try:
        print(f"ğŸ“– è®€å– Excel æª”æ¡ˆï¼š{xlsx_path.name}")
        wb = openpyxl.load_workbook(xlsx_path, data_only=True)
        ws = wb.active
        
        header_row = list(ws[1])
        header = {cell.value: idx for idx, cell in enumerate(header_row) if cell.value}
        
        # åŸºæœ¬æ¬„ä½æª¢æŸ¥
        required_columns = ["æª”æ¡ˆé¡å‹", "é …ç›®ID", "é …ç›®å…§å®¹"]
        missing_columns = []
        
        for col in required_columns:
            if col not in header:
                missing_columns.append(col)
        
        if missing_columns:
            print(f"âŒ Excel ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}")
            return {}
        
        # è‡ªå‹•æª¢æ¸¬æ‰€æœ‰æ¥­æ…‹çš„æ›¿æ›çµæœæ¬„ä½
        business_types = config.get_business_types()
        available_business_types = []
        
        for bt_code, bt_config in business_types.items():
            display_name = bt_config['display_name']
            result_col_name = f"{display_name}_æ›¿æ›çµæœ"
            if result_col_name in header:
                available_business_types.append(bt_code)
        
        if not available_business_types:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¥­æ…‹çš„æ›¿æ›çµæœæ¬„ä½")
            return {}
        
        print(f"   ğŸ“‹ æª¢æ¸¬åˆ°æ¥­æ…‹ï¼š{', '.join([business_types[bt]['display_name'] for bt in available_business_types])}")
        
        # è§£ææ›´æ–°è³‡æ–™
        updates = {bt_code: {"po": [], "json": []} for bt_code in available_business_types}
        
        for row_num, row in enumerate(ws.iter_rows(min_row=2, values_only=True), start=2):
            if not row or len(row) <= max(header.values()):
                continue
            
            try:
                file_type = row[header["æª”æ¡ˆé¡å‹"]]
                entry_id = row[header["é …ç›®ID"]]
                original_text = row[header["é …ç›®å…§å®¹"]]
                
                if not file_type or not entry_id:
                    continue
                
                file_type = str(file_type).lower()
                
                # è™•ç†æ¯å€‹å¯ç”¨çš„æ¥­æ…‹
                for bt_code in available_business_types:
                    display_name = business_types[bt_code]['display_name']
                    result_col_name = f"{display_name}_æ›¿æ›çµæœ"
                    
                    new_value = row[header[result_col_name]]
                    
                    # è·³éç©ºå€¼å’Œèˆ‡åŸæ–‡ç›¸åŒçš„å€¼
                    if not new_value or not str(new_value).strip():
                        continue
                    
                    new_value = str(new_value).strip()
                    
                    if original_text and str(original_text).strip() == new_value:
                        continue
                    
                    # å‰µå»ºæ›´æ–°è¨˜éŒ„
                    update_record = (str(entry_id), new_value)
                    
                    if file_type == "po":
                        updates[bt_code]["po"].append(update_record)
                    elif file_type == "json":
                        updates[bt_code]["json"].append(update_record)
            
            except Exception as e:
                print(f"âš ï¸  ç¬¬ {row_num} è¡Œè™•ç†å¤±æ•—: {e}")
                continue
        
        # çµ±è¨ˆæœ‰æ•ˆæ›´æ–°
        total_updates = 0
        for bt_code in available_business_types:
            bt_updates = len(updates[bt_code]["po"]) + len(updates[bt_code]["json"])
            total_updates += bt_updates
            if bt_updates > 0:
                print(f"     {business_types[bt_code]['display_name']}: {bt_updates} å€‹æ›´æ–°")
        
        print(f"   ğŸ“Š ç¸½è¨ˆï¼š{total_updates} å€‹æœ‰æ•ˆæ›´æ–°")
        return updates
        
    except Exception as e:
        print(f"âŒ è®€å– Excel æª”æ¡ˆå¤±æ•—ï¼š{e}")
        return {}


def combine_po_files(updates_list: list, target_po_path: Path, output_po_path: Path) -> dict:
    """åˆä½µ PO æª”æ¡ˆ"""
    result = {
        "success": False,
        "merged": 0,
        "skipped": 0,
        "conflicts": [],
        "errors": []
    }
    
    if not updates_list:
        result["success"] = True
        return result
    
    try:
        # è¼‰å…¥ç›®æ¨™ PO æª”æ¡ˆ
        if not target_po_path.exists():
            result["errors"].append(f"ç›®æ¨™ PO æª”æ¡ˆä¸å­˜åœ¨ï¼š{target_po_path}")
            return result
        
        target_po = polib.pofile(str(target_po_path))
        print(f"   ğŸ“„ è¼‰å…¥ç›®æ¨™ PO æª”æ¡ˆï¼š{target_po_path.name}ï¼Œå…± {len(target_po)} å€‹æ¢ç›®")
        
        conflicts = []
        
        # è™•ç†æ›´æ–°
        for msgid, new_msgstr in updates_list:
            target_entry = target_po.find(msgid)
            
            if target_entry:
                # æª¢æŸ¥æ˜¯å¦æœ‰è¡çª
                if target_entry.msgstr and target_entry.msgstr.strip():
                    if target_entry.msgstr != new_msgstr:
                        # ç™¼ç¾è¡çª
                        conflict_info = {
                            'msgid': msgid,
                            'existing_value': target_entry.msgstr,
                            'new_value': new_msgstr,
                            'file_type': 'po'
                        }
                        conflicts.append(conflict_info)
                        continue
                    else:
                        # å€¼ç›¸åŒï¼Œè·³é
                        result["skipped"] += 1
                        continue
                
                # æ‡‰ç”¨æ›´æ–°
                target_entry.msgstr = new_msgstr
                result["merged"] += 1
            else:
                # ç›®æ¨™æª”æ¡ˆä¸­æ²’æœ‰æ­¤æ¢ç›®ï¼Œæ·»åŠ æ–°æ¢ç›®
                new_entry = polib.POEntry(
                    msgid=msgid,
                    msgstr=new_msgstr
                )
                target_po.append(new_entry)
                result["merged"] += 1
        
        # å¦‚æœæœ‰è¡çªï¼Œè¨˜éŒ„ä½†ä¸å„²å­˜
        if conflicts:
            result["conflicts"] = conflicts
            result["success"] = False
            return result
        
        # ä¿å­˜åˆä½µå¾Œçš„æª”æ¡ˆ
        output_po_path.parent.mkdir(parents=True, exist_ok=True)
        target_po.save(str(output_po_path))
        
        result["success"] = True
        
    except Exception as e:
        result["errors"].append(f"PO æª”æ¡ˆåˆä½µå¤±æ•—ï¼š{e}")
    
    return result


def combine_json_files(updates_list: list, target_json_path: Path, output_json_path: Path) -> dict:
    """åˆä½µ JSON æª”æ¡ˆ"""
    result = {
        "success": False,
        "merged": 0,
        "skipped": 0,
        "conflicts": [],
        "errors": []
    }
    
    if not updates_list:
        result["success"] = True
        return result
    
    try:
        # è¼‰å…¥ç›®æ¨™ JSON æª”æ¡ˆ
        if not target_json_path.exists():
            result["errors"].append(f"ç›®æ¨™ JSON æª”æ¡ˆä¸å­˜åœ¨ï¼š{target_json_path}")
            return result
        
        target_data = json.loads(target_json_path.read_text(encoding="utf-8"))
        print(f"   ğŸ“„ è¼‰å…¥ç›®æ¨™ JSON æª”æ¡ˆï¼š{target_json_path.name}")
        
        conflicts = []
        
        # è™•ç†æ›´æ–°
        for json_path_str, new_value in updates_list:
            # ç²å–ç¾æœ‰å€¼
            existing_value = get_json_value_by_path(target_data, json_path_str)
            
            if existing_value is not None:
                # æª¢æŸ¥æ˜¯å¦æœ‰è¡çª
                if str(existing_value).strip() != str(new_value).strip():
                    # ç™¼ç¾è¡çª
                    conflict_info = {
                        'path': json_path_str,
                        'existing_value': existing_value,
                        'new_value': new_value,
                        'file_type': 'json'
                    }
                    conflicts.append(conflict_info)
                    continue
                else:
                    # å€¼ç›¸åŒï¼Œè·³é
                    result["skipped"] += 1
                    continue
            
            # æ‡‰ç”¨æ›´æ–°
            if set_json_value_by_path(target_data, json_path_str, new_value):
                result["merged"] += 1
            else:
                result["errors"].append(f"ç„¡æ³•è¨­ç½® JSON è·¯å¾‘ï¼š{json_path_str}")
        
        # å¦‚æœæœ‰è¡çªï¼Œè¨˜éŒ„ä½†ä¸å„²å­˜
        if conflicts:
            result["conflicts"] = conflicts
            result["success"] = False
            return result
        
        # ä¿å­˜åˆä½µå¾Œçš„æª”æ¡ˆ
        output_json_path.parent.mkdir(parents=True, exist_ok=True)
        
        json_content = json.dumps(target_data, ensure_ascii=False, indent=2)
        output_json_path.write_text(json_content, encoding="utf-8")
        
        result["success"] = True
        
    except json.JSONDecodeError as e:
        result["errors"].append(f"JSON æ ¼å¼éŒ¯èª¤ï¼š{e}")
    except Exception as e:
        result["errors"].append(f"JSON æª”æ¡ˆåˆä½µå¤±æ•—ï¼š{e}")
    
    return result


def get_json_value_by_path(data: dict, path: str):
    """æŒ‰è·¯å¾‘ç²å– JSON å€¼"""
    try:
        path_parts = parse_json_path(path)
        current = data
        
        for part_type, part_value in path_parts:
            if part_type == 'key':
                if part_value not in current:
                    return None
                current = current[part_value]
            elif part_type == 'index':
                if not isinstance(current, list) or len(current) <= part_value:
                    return None
                current = current[part_value]
        
        return current
        
    except Exception:
        return None


def set_json_value_by_path(data: dict, path: str, new_value: str) -> bool:
    """æŒ‰è·¯å¾‘è¨­ç½® JSON å€¼"""
    try:
        path_parts = parse_json_path(path)
        current = data
        
        for i, (part_type, part_value) in enumerate(path_parts):
            is_last = (i == len(path_parts) - 1)
            
            if part_type == 'key':
                if is_last:
                    current[part_value] = new_value
                else:
                    if part_value not in current:
                        next_part_type = path_parts[i + 1][0] if i + 1 < len(path_parts) else 'key'
                        current[part_value] = [] if next_part_type == 'index' else {}
                    current = current[part_value]
            
            elif part_type == 'index':
                if is_last:
                    while len(current) <= part_value:
                        current.append(None)
                    current[part_value] = new_value
                else:
                    while len(current) <= part_value:
                        current.append(None)
                    if current[part_value] is None:
                        next_part_type = path_parts[i + 1][0] if i + 1 < len(path_parts) else 'key'
                        current[part_value] = [] if next_part_type == 'index' else {}
                    current = current[part_value]
        
        return True
        
    except Exception as e:
        return False


def parse_json_path(path: str) -> list:
    """è§£æ JSON è·¯å¾‘"""
    parts = []
    current = ""
    in_bracket = False
    
    for char in path:
        if char == '[':
            if current:
                parts.append(('key', current))
                current = ""
            in_bracket = True
        elif char == ']':
            if in_bracket and current:
                try:
                    parts.append(('index', int(current)))
                except ValueError:
                    raise ValueError(f"ç„¡æ•ˆçš„é™£åˆ—ç´¢å¼•ï¼š{current}")
                current = ""
            in_bracket = False
        elif char == '.' and not in_bracket:
            if current:
                parts.append(('key', current))
                current = ""
        else:
            current += char
    
    if current:
        parts.append(('key', current))
    
    return parts


def choose_business_types(config) -> list:
    """é¸æ“‡è¦è™•ç†çš„æ¥­æ…‹ - å·²ç§»é™¤ï¼Œæ”¹ç‚ºè‡ªå‹•è™•ç†æ‰€æœ‰æœ‰æ›¿æ›çµæœçš„æ¥­æ…‹"""
    # æ­¤å‡½æ•¸å·²ä¸å†ä½¿ç”¨ï¼Œä¿ç•™ä»¥ç¶­æŒç›¸å®¹æ€§
    business_types = config.get_business_types()
    return list(business_types.keys())


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸš€ é–‹å§‹æª”æ¡ˆåˆä½µè™•ç† (v1.0)")
    
    # è¼‰å…¥é…ç½®
    config = get_config()
    
    # æª¢æ¸¬å¯ç”¨çš„ tobemodified æª”æ¡ˆ
    available_files = detect_tobemodified_files(config)
    
    if not available_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• tobemodified æª”æ¡ˆ")
        print("è«‹å…ˆåŸ·è¡Œ script_01_generate_xlsx.py ç”Ÿæˆæª”æ¡ˆ")
        sys.exit(1)
    
    # æ­¥é©Ÿ1ï¼šé¸æ“‡ tobemodified æª”æ¡ˆ
    language, tobemodified_path = choose_tobemodified_file(available_files)
    if not language:
        sys.exit(1)
    
    # æª¢æŸ¥ i18n_combine ç›®éŒ„
    combine_dir = Path("i18n_combine")
    
    if not combine_dir.exists():
        print(f"âŒ åˆä½µç›®éŒ„ä¸å­˜åœ¨ï¼š{combine_dir}")
        print(f"è«‹å‰µå»ºç›®éŒ„ä¸¦æ”¾å…¥è¦åˆä½µçš„æª”æ¡ˆ")
        sys.exit(1)
    
    print(f"ğŸ“ æƒæåˆä½µç›®éŒ„ï¼š{combine_dir}")
    
    # æƒæ combine ç›®éŒ„ä¸­çš„æª”æ¡ˆ
    combine_files = scan_combine_directory(combine_dir)
    
    # æ­¥é©Ÿ2ï¼šé¸æ“‡è¦åˆä½µçš„ JSON æª”æ¡ˆ
    target_json_path = choose_combine_file(combine_files['json'], 'json')
    
    # æ­¥é©Ÿ3ï¼šé¸æ“‡è¦åˆä½µçš„ PO æª”æ¡ˆ
    target_po_path = choose_combine_file(combine_files['po'], 'po')
    
    # æª¢æŸ¥æ˜¯å¦è‡³å°‘é¸æ“‡äº†ä¸€å€‹æª”æ¡ˆ
    if not target_json_path and not target_po_path:
        print("âŒ å¿…é ˆè‡³å°‘é¸æ“‡ä¸€å€‹æª”æ¡ˆé€²è¡Œåˆä½µ")
        sys.exit(1)
    
    # è®€å– Excel æ›´æ–°è³‡æ–™ï¼ˆè‡ªå‹•æª¢æ¸¬æ‰€æœ‰æ¥­æ…‹ï¼‰
    updates = read_excel_updates(tobemodified_path, config)
    if not updates:
        print("âŒ è®€å– Excel æª”æ¡ˆå¤±æ•—æˆ–æ²’æœ‰æœ‰æ•ˆçš„æ›´æ–°")
        sys.exit(1)
    
    # ç²å–å¯¦éš›æœ‰æ›´æ–°çš„æ¥­æ…‹
    target_business_types = [bt_code for bt_code, bt_updates in updates.items() 
                            if bt_updates['po'] or bt_updates['json']]
    
    if not target_business_types:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ¥­æ…‹æ›´æ–°")
        sys.exit(1)
    
    print(f"\nğŸ“‹ åˆä½µè¨­å®šï¼š")
    print(f"   ä¾†æºæª”æ¡ˆï¼š{tobemodified_path.name}")
    if target_json_path:
        print(f"   JSON æª”æ¡ˆï¼š{target_json_path.relative_to(combine_dir)}")
    if target_po_path:
        print(f"   PO æª”æ¡ˆï¼š{target_po_path.relative_to(combine_dir)}")
    print(f"   ç›®æ¨™æ¥­æ…‹ï¼š{', '.join([config.get_business_types()[bt]['display_name'] for bt in target_business_types])}")
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    dirs = config.get_directories()
    output_dir = Path(dirs['output_dir']) / f"{language}_{timestamp}_combined"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¨­ç½®æ—¥èªŒ
    log_file = output_dir / f"combine_{timestamp}.log"
    
    def log_detail(message: str):
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"{datetime.datetime.now().strftime('%H:%M:%S')} - {message}\n")
    
    log_detail(f"é–‹å§‹åˆä½µè™•ç†")
    log_detail(f"èªè¨€ï¼š{language}")
    log_detail(f"ä¾†æºæª”æ¡ˆï¼š{tobemodified_path}")
    log_detail(f"ç›®æ¨™æ¥­æ…‹ï¼š{', '.join(target_business_types)}")
    
    # è™•ç†æ¯å€‹æ¥­æ…‹
    business_types = config.get_business_types()
    all_results = {}
    has_conflicts = False
    
    for bt_code in target_business_types:
        bt_config = business_types[bt_code]
        display_name = bt_config['display_name']
        suffix = bt_config['suffix']
        
        print(f"\nğŸ“ è™•ç† {display_name}...")
        log_detail(f"é–‹å§‹è™•ç†æ¥­æ…‹ï¼š{display_name}")
        
        results = {}
        
        # è™•ç† PO æª”æ¡ˆ
        if target_po_path and updates[bt_code]['po']:
            output_po_path = output_dir / f"{target_po_path.stem}{suffix}_combined.po"
            po_result = combine_po_files(
                updates[bt_code]['po'],
                target_po_path,
                output_po_path
            )
            results['po_result'] = po_result
            
            if po_result['conflicts']:
                has_conflicts = True
                print(f"     âŒ PO æª”æ¡ˆç™¼ç¾ {len(po_result['conflicts'])} å€‹è¡çª")
                for conflict in po_result['conflicts']:
                    print(f"       è¡çª msgid: '{conflict['msgid']}'")
                    print(f"         ç¾æœ‰å€¼: '{conflict['existing_value']}'")
                    print(f"         æ–°å€¼: '{conflict['new_value']}'")
        else:
            # å³ä½¿æ²’æœ‰æ›´æ–°ä¹Ÿè¤‡è£½åŸæª”æ¡ˆ
            if target_po_path:
                output_po_path = output_dir / f"{target_po_path.stem}{suffix}_combined.po"
                output_po_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(target_po_path, output_po_path)
                print(f"     ğŸ“„ è¤‡è£½ PO æª”æ¡ˆï¼ˆç„¡æ›´æ–°ï¼‰")
        
        # è™•ç† JSON æª”æ¡ˆ
        if target_json_path and updates[bt_code]['json']:
            output_json_path = output_dir / f"{target_json_path.stem}{suffix}_combined.json"
            json_result = combine_json_files(
                updates[bt_code]['json'],
                target_json_path,
                output_json_path
            )
            results['json_result'] = json_result
            
            if json_result['conflicts']:
                has_conflicts = True
                print(f"     âŒ JSON æª”æ¡ˆç™¼ç¾ {len(json_result['conflicts'])} å€‹è¡çª")
                for conflict in json_result['conflicts']:
                    print(f"       è¡çªè·¯å¾‘: '{conflict['path']}'")
                    print(f"         ç¾æœ‰å€¼: '{conflict['existing_value']}'")
                    print(f"         æ–°å€¼: '{conflict['new_value']}'")
        else:
            # å³ä½¿æ²’æœ‰æ›´æ–°ä¹Ÿè¤‡è£½åŸæª”æ¡ˆ
            if target_json_path:
                output_json_path = output_dir / f"{target_json_path.stem}{suffix}_combined.json"
                output_json_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(target_json_path, output_json_path)
        
        all_results[bt_code] = results
        
        # çµ±è¨ˆçµæœ
        total_merged = 0
        total_skipped = 0
        
        for result in results.values():
            total_merged += result.get('merged', 0)
            total_skipped += result.get('skipped', 0)
        
        if not has_conflicts:
            print(f"     âœ… å®Œæˆ - åˆä½µ: {total_merged} å€‹, è·³é: {total_skipped} å€‹")
        
        log_detail(f"{display_name} è™•ç†å®Œæˆï¼šåˆä½µ {total_merged} å€‹ï¼Œè·³é {total_skipped} å€‹")"         ç¾æœ‰å€¼: '{conflict['existing_value']}'")
                    print(f"         æ–°å€¼: '{conflict['new_value']}'")
        else:
            # å³ä½¿æ²’æœ‰æ›´æ–°ä¹Ÿè¤‡è£½åŸæª”æ¡ˆ
            if target_json_path:
                output_json_path = output_dir / f"{target_json_path.stem}{suffix}.json"
                output_json_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(target_json_path, output_json_path)
                print(f"     ğŸ“„ è¤‡è£½ JSON æª”æ¡ˆï¼ˆç„¡æ›´æ–°ï¼‰")
        
        all_results[bt_code] = results
        
        # çµ±è¨ˆçµæœ
        total_merged = 0
        total_skipped = 0
        
        for result in results.values():
            total_merged += result.get('merged', 0)
            total_skipped += result.get('skipped', 0)
        
        if not has_conflicts:
            print(f"     âœ… å®Œæˆ - åˆä½µ: {total_merged} å€‹, è·³é: {total_skipped} å€‹")
        
        log_detail(f"{display_name} è™•ç†å®Œæˆï¼šåˆä½µ {total_merged} å€‹ï¼Œè·³é {total_skipped} å€‹")
    
    # å¦‚æœæœ‰è¡çªï¼Œçµ‚æ­¢æ“ä½œ
    if has_conflicts:
        print(f"\nâŒ ç™¼ç¾è¡çªï¼Œæ“ä½œå·²çµ‚æ­¢")
        print(f"è«‹æª¢æŸ¥ä¸¦è§£æ±ºè¡çªå¾Œé‡æ–°åŸ·è¡Œ")
        log_detail(f"è™•ç†å› è¡çªè€Œçµ‚æ­¢")
        sys.exit(1)
    
    # ç”Ÿæˆæœ€çµ‚å ±å‘Š
    total_merged = sum(
        sum(result.get('merged', 0) for result in results.values())
        for results in all_results.values()
    )
    total_skipped = sum(
        sum(result.get('skipped', 0) for result in results.values())
        for results in all_results.values()
    )
    
    print(f"\nğŸ‰ åˆä½µè™•ç†å®Œæˆï¼")
    print(f"ğŸ“Š è™•ç†çµæœï¼šåˆä½µ {total_merged} å€‹é …ç›®ï¼Œè·³é {total_skipped} å€‹é …ç›®")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„ï¼š{output_dir}")
    
    # ç”Ÿæˆè™•ç†æ‘˜è¦
    generate_combine_summary_report(all_results, output_dir, timestamp, target_json_path, target_po_path, log_detail)


def generate_combine_summary_report(results: dict, output_dir: Path, timestamp: str, 
                                   target_json_path: Path, target_po_path: Path, log_detail):
    """ç”Ÿæˆåˆä½µè™•ç†æ‘˜è¦å ±å‘Š"""
    summary_file = output_dir / f"combine_summary_{timestamp}.txt"
    
    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"æª”æ¡ˆåˆä½µè™•ç†æ‘˜è¦å ±å‘Š\n")
            f.write(f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*50}\n\n")
            
            f.write(f"ç›®æ¨™æª”æ¡ˆï¼š\n")
            if target_json_path:
                f.write(f"  JSON: {target_json_path}\n")
            if target_po_path:
                f.write(f"  PO: {target_po_path}\n")
            f.write(f"\n")
            
            total_merged = 0
            total_skipped = 0
            successful_business_types = []
            failed_business_types = []
            
            for bt_code, bt_results in results.items():
                f.write(f"æ¥­æ…‹ï¼š{bt_code}\n")
                
                bt_merged = sum(result.get('merged', 0) for result in bt_results.values())
                bt_skipped = sum(result.get('skipped', 0) for result in bt_results.values())
                bt_errors = []
                for result in bt_results.values():
                    bt_errors.extend(result.get('errors', []))
                
                f.write(f"åˆä½µæ•¸é‡ï¼š{bt_merged}\n")
                f.write(f"è·³éæ•¸é‡ï¼š{bt_skipped}\n")
                
                if bt_errors:
                    f.write(f"éŒ¯èª¤ï¼š\n")
                    for error in bt_errors:
                        f.write(f"  - {error}\n")
                    failed_business_types.append(bt_code)
                else:
                    successful_business_types.append(bt_code)
                    total_merged += bt_merged
                    total_skipped += bt_skipped
                
                f.write(f"\n{'-'*30}\n\n")
            
            # ç¸½è¨ˆçµ±è¨ˆ
            f.write(f"è™•ç†ç¸½çµï¼š\n")
            f.write(f"æˆåŠŸæ¥­æ…‹ï¼š{len(successful_business_types)}\n")
            f.write(f"å¤±æ•—æ¥­æ…‹ï¼š{len(failed_business_types)}\n")
            f.write(f"ç¸½åˆä½µé …ç›®ï¼š{total_merged}\n")
            f.write(f"ç¸½è·³éé …ç›®ï¼š{total_skipped}\n")
            
            if successful_business_types:
                f.write(f"\næˆåŠŸçš„æ¥­æ…‹ï¼š{', '.join(successful_business_types)}\n")
            
            if failed_business_types:
                f.write(f"å¤±æ•—çš„æ¥­æ…‹ï¼š{', '.join(failed_business_types)}\n")
            
            f.write(f"\nåˆä½µèªªæ˜ï¼š\n")
            f.write(f"- æœ¬æ¬¡è™•ç†å°‡ tobemodified ä¸­çš„æ›¿æ›çµæœåˆä½µåˆ°æŒ‡å®šæª”æ¡ˆ\n")
            f.write(f"- ç›¸åŒ key ä¸”ç›¸åŒ value çš„é …ç›®æœƒè‡ªå‹•è·³é\n")
            f.write(f"- ç›¸åŒ key ä½†ä¸åŒ value çš„é …ç›®æœƒç”¢ç”Ÿè¡çªä¸¦ä¸­æ–·æ“ä½œ\n")
            f.write(f"- åˆä½µæˆåŠŸçš„æª”æ¡ˆä¿å­˜åœ¨å¸¶æ™‚é–“æˆ³çš„ç›®éŒ„ä¸­\n")
            
            f.write(f"\nä½¿ç”¨å»ºè­°ï¼š\n")
            f.write(f"- å¦‚ç™¼ç¾è¡çªï¼Œè«‹æª¢æŸ¥ä¸¦æ‰‹å‹•è§£æ±ºå¾Œé‡æ–°åŸ·è¡Œ\n")
            f.write(f"- åˆä½µå‰å»ºè­°å‚™ä»½åŸå§‹æª”æ¡ˆ\n")
            f.write(f"- åˆä½µå¾Œè«‹æ¸¬è©¦ç¿»è­¯æª”æ¡ˆçš„æ­£ç¢ºæ€§\n")
        
        log_detail(f"åˆä½µæ‘˜è¦å ±å‘Šå·²ç”Ÿæˆï¼š{summary_file}")
        
    except Exception as e:
        log_detail(f"ç”Ÿæˆåˆä½µæ‘˜è¦å ±å‘Šå¤±æ•—ï¼š{e}")


if __name__ == "__main__":
    main()