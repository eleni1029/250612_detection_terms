#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
script_02_apply_combine.py (v1.6 - æ”¯æ´é™£åˆ—å®Œæ•´æ›´æ–°ç‰ˆ)

åŠŸèƒ½ï¼š
1. é¸æ“‡è¦åˆä½µçš„ tobemodified Excel æª”æ¡ˆï¼ˆæ”¯æ´å¤šé¸ï¼‰
2. é¸æ“‡ i18n_combine ç›®éŒ„ä¸‹çš„ JSON/PO æª”æ¡ˆä½œç‚ºåˆä½µç›®æ¨™
3. æŒ‰æ¥­æ…‹åˆ†åˆ¥è™•ç†ï¼Œé¿å…ç›¸äº’è¡çª
4. æ²’æœ‰ç›®æ¨™æª”æ¡ˆæ™‚è‡ªå‹•å‰µå»ºæ¨™æº–æª”æ¡ˆï¼ˆJSON/POï¼‰
5. ç”Ÿæˆåˆä½µå¾Œçš„æª”æ¡ˆåˆ° i18n_output/multi_{timestamp}_combined/
6. æä¾›è©³ç´°çš„åˆä½µå ±å‘Šå’Œæ—¥èªŒ
7. **æ–°å¢ï¼šå®Œæ•´é™£åˆ—æ›´æ–°é‚è¼¯ - å¾ i18n_input è®€å–åŸå§‹é™£åˆ—é€²è¡Œæ™ºèƒ½åˆä½µ**
"""

import json
import sys
import shutil
import datetime
import argparse
import glob
from pathlib import Path
from collections import defaultdict
from config_loader import get_config

try:
    import openpyxl
    import polib
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦å¥—ä»¶ï¼š{e}")
    print("è«‹åŸ·è¡Œï¼špip install openpyxl polib")
    sys.exit(1)


def check_multilang_json_structure(data: dict) -> bool:
    """æª¢æŸ¥ JSON æ˜¯å¦ç‚ºå¤šèªè¨€çµæ§‹ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
    if not isinstance(data, dict):
        return False
    
    # ç°¡åŒ–çš„æª¢æŸ¥ï¼šå¦‚æœé ‚å±¤ key çœ‹èµ·ä¾†åƒèªè¨€ä»£ç¢¼ï¼ˆ2-5å€‹å­—ç¬¦ï¼‰ï¼Œå‰‡èªç‚ºæ˜¯å¤šèªè¨€çµæ§‹
    for key in data.keys():
        if isinstance(key, str) and 2 <= len(key) <= 10 and isinstance(data[key], dict):
            return True
    
    return False


def load_original_language_json(language: str) -> dict:
    """è¼‰å…¥æŒ‡å®šèªè¨€çš„åŸå§‹ JSON æª”æ¡ˆ (i18n_input/{language}/{language}.json)"""
    try:
        input_dir = Path("i18n_input")
        language_file = input_dir / language / f"{language}.json"
        
        if not language_file.exists():
            print(f"âš ï¸  åŸå§‹èªè¨€æª”æ¡ˆä¸å­˜åœ¨ï¼š{language_file}")
            return {}
        
        with open(language_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"âœ… è¼‰å…¥åŸå§‹èªè¨€æª”æ¡ˆï¼š{language_file}")
            return data
    
    except Exception as e:
        print(f"âŒ è¼‰å…¥åŸå§‹èªè¨€æª”æ¡ˆå¤±æ•— ({language})ï¼š{e}")
        return {}


def detect_array_path_and_index(path: str) -> tuple:
    """
    æª¢æ¸¬è·¯å¾‘æ˜¯å¦åŒ…å«é™£åˆ—ç´¢å¼•ï¼Œä¸¦è¿”å›é™£åˆ—è·¯å¾‘å’Œç´¢å¼•
    
    Returns:
        (array_path, index) å¦‚æœæ˜¯é™£åˆ—ç´¢å¼•è·¯å¾‘
        (None, None) å¦‚æœä¸æ˜¯é™£åˆ—ç´¢å¼•è·¯å¾‘
    
    ä¾‹å¦‚ï¼š
        "slogan[1]" -> ("slogan", 1)
        "data.items[0].tags[2]" -> ("data.items[0].tags", 2)
        "simple.key" -> (None, None)
    """
    import re
    
    # ä½¿ç”¨æ­£è¦è¡¨é”å¼æ‰¾åˆ°æœ€å¾Œä¸€å€‹é™£åˆ—ç´¢å¼•
    pattern = r'^(.+)\[(\d+)\]$'
    match = re.match(pattern, path)
    
    if match:
        array_path = match.group(1)
        index = int(match.group(2))
        return (array_path, index)
    
    return (None, None)


def get_array_from_original_json(original_data: dict, array_path: str) -> list:
    """å¾åŸå§‹ JSON è³‡æ–™ä¸­ç²å–æŒ‡å®šè·¯å¾‘çš„é™£åˆ—"""
    try:
        path_parts = parse_json_path(array_path)
        current = original_data
        
        for part_type, part_value in path_parts:
            if part_type == 'key':
                if part_value not in current:
                    print(f"âš ï¸  åŸå§‹è³‡æ–™ä¸­æ‰¾ä¸åˆ°è·¯å¾‘ï¼š{array_path}")
                    return []
                current = current[part_value]
            elif part_type == 'index':
                if not isinstance(current, list) or len(current) <= part_value:
                    print(f"âš ï¸  åŸå§‹è³‡æ–™ä¸­é™£åˆ—ç´¢å¼•è¶…å‡ºç¯„åœï¼š{array_path}")
                    return []
                current = current[part_value]
        
        if isinstance(current, list):
            return current.copy()  # è¿”å›å‰¯æœ¬é¿å…ä¿®æ”¹åŸå§‹è³‡æ–™
        else:
            print(f"âš ï¸  æŒ‡å®šè·¯å¾‘ä¸æ˜¯é™£åˆ—ï¼š{array_path} (é¡å‹: {type(current)})")
            return []
            
    except Exception as e:
        print(f"âŒ å¾åŸå§‹è³‡æ–™ç²å–é™£åˆ—å¤±æ•—ï¼š{array_path} - {e}")
        return []


def create_default_json_file(output_path: Path, all_updates: dict, detected_languages: list) -> bool:
    """å‰µå»ºé è¨­çš„å¤šèªè¨€ JSON æª”æ¡ˆï¼ˆåƒ…åŒ…å«æª¢æ¸¬åˆ°çš„èªè¨€å€å¡Šï¼‰"""
    try:
        # æ ¹æ“šæª¢æ¸¬åˆ°çš„èªè¨€å»ºç«‹ç©ºçµæ§‹
        json_data = {}
        
        # åªæ·»åŠ æª¢æ¸¬åˆ°çš„èªè¨€ï¼Œå‰µå»ºç©ºçµæ§‹
        for language in detected_languages:
            json_data[language] = {}
        
        # æ ¹æ“š Excel æ›´æ–°è³‡æ–™å‹•æ…‹æ·»åŠ è·¯å¾‘çµæ§‹ï¼ˆä½†ä¸è¨­ç½®å€¼ï¼‰
        for language, language_updates in all_updates.items():
            if language not in json_data:
                json_data[language] = {}
                
            for bt_code, bt_updates in language_updates.items():
                for json_path_str, new_value, update_language in bt_updates['json']:
                    # ç¢ºä¿è·¯å¾‘å­˜åœ¨æ–¼å°æ‡‰èªè¨€ä¸­
                    if update_language in json_data:
                        # é å…ˆå‰µå»ºè·¯å¾‘çµæ§‹ï¼Œä½†ä¸è¨­ç½®å€¼ï¼ˆå°‡ç”±å¾ŒçºŒåˆä½µè™•ç†ï¼‰
                        create_json_path_structure(json_data[update_language], json_path_str)
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æª”æ¡ˆ
        json_content = json.dumps(json_data, ensure_ascii=False, indent=2)
        output_path.write_text(json_content, encoding="utf-8")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‰µå»ºé è¨­ JSON æª”æ¡ˆå¤±æ•—ï¼š{e}")
        return False


def create_json_path_structure(data: dict, path: str):
    """åœ¨ JSON ä¸­é å…ˆå‰µå»ºè·¯å¾‘çµæ§‹"""
    try:
        path_parts = parse_json_path(path)
        current = data
        
        for i, (part_type, part_value) in enumerate(path_parts):
            is_last = (i == len(path_parts) - 1)
            
            if part_type == 'key':
                if not is_last:
                    if part_value not in current:
                        # æª¢æŸ¥ä¸‹ä¸€éƒ¨åˆ†æ˜¯å¦ç‚ºç´¢å¼•
                        next_part_type = path_parts[i + 1][0] if i + 1 < len(path_parts) else 'key'
                        current[part_value] = [] if next_part_type == 'index' else {}
                    current = current[part_value]
                else:
                    # æœ€å¾Œä¸€å€‹éƒ¨åˆ†ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¨­ç‚ºç©ºå­—ä¸²
                    if part_value not in current:
                        current[part_value] = ""
            
            elif part_type == 'index':
                if not is_last:
                    while len(current) <= part_value:
                        current.append({})
                    current = current[part_value]
                else:
                    while len(current) <= part_value:
                        current.append("")
        
    except Exception as e:
        print(f"âš ï¸  å‰µå»ºJSONè·¯å¾‘çµæ§‹å¤±æ•—ï¼š{path} - {e}")


def create_default_po_file(output_path: Path, language: str = "zh_Hant_TW") -> bool:
    """å‰µå»ºé è¨­çš„ messages.po æª”æ¡ˆï¼ˆåƒ…åŒ…å«æ¨™é ­ï¼Œç„¡ç¯„ä¾‹æ¢ç›®ï¼‰"""
    try:
        # å‰µå»ºæ–°çš„ PO æª”æ¡ˆ
        po = polib.POFile()
        
        # è¨­ç½®æ¨™é ­è³‡è¨Š
        current_time = datetime.datetime.now()
        po.metadata = {
            'Project-Id-Version': 'PROJECT VERSION',
            'Report-Msgid-Bugs-To': 'EMAIL@ADDRESS',
            'POT-Creation-Date': current_time.strftime('%Y-%m-%d %H:%M%z'),
            'PO-Revision-Date': 'YEAR-MO-DA HO:MI+ZONE',
            'Last-Translator': 'FULL NAME <EMAIL@ADDRESS>',
            'Language': language,
            'Language-Team': f'{language} <LL@li.org>',
            'Plural-Forms': 'nplurals=1; plural=0;',
            'MIME-Version': '1.0',
            'Content-Type': 'text/plain; charset=utf-8',
            'Content-Transfer-Encoding': '8bit',
            'Generated-By': 'Babel 2.12.1'
        }
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æª”æ¡ˆ
        po.save(str(output_path))
        
        return True
        
    except Exception as e:
        print(f"âŒ å‰µå»ºé è¨­ PO æª”æ¡ˆå¤±æ•—ï¼š{e}")
        return False


def detect_tobemodified_files(config) -> dict:
    """æª¢æ¸¬å¯ç”¨çš„ tobemodified æª”æ¡ˆ"""
    available_files = {}
    
    # æª¢æ¸¬è¼¸å‡ºç›®éŒ„ä¸­çš„æª”æ¡ˆ
    try:
        dirs = config.get_directories()
        output_dir = Path(dirs['output_dir'])
    except Exception:
        output_dir = Path('i18n_output')
    
    # ä½¿ç”¨é…ç½®è¼‰å…¥å™¨çš„èªè¨€æª¢æ¸¬
    try:
        available_languages = config.detect_available_languages()
    except Exception as e:
        print(f"âš ï¸  èªè¨€æª¢æ¸¬å¤±æ•—ï¼š{e}")
        available_languages = []
    
    # æª¢æ¸¬æ¨™æº–å‘½åçš„æª”æ¡ˆ
    for language in available_languages:
        tobemodified_path = output_dir / f"{language}_tobemodified.xlsx"
        if tobemodified_path.exists():
            available_files[language] = tobemodified_path
    
    return available_files


def scan_combine_directory(combine_dir: Path) -> dict:
    """æƒæ i18n_combine ç›®éŒ„ä¸­çš„æª”æ¡ˆ"""
    files = {
        'json': [],
        'po': []
    }
    
    if not combine_dir.exists():
        return files
    
    # éæ­¸æƒææ‰€æœ‰ JSON å’Œ PO æª”æ¡ˆ
    for file_path in combine_dir.rglob("*.json"):
        relative_path = file_path.relative_to(combine_dir)
        files['json'].append({
            'path': file_path,
            'relative_path': str(relative_path),
            'name': file_path.name
        })
    
    for file_path in combine_dir.rglob("*.po"):
        relative_path = file_path.relative_to(combine_dir)
        files['po'].append({
            'path': file_path,
            'relative_path': str(relative_path),
            'name': file_path.name
        })
    
    return files


def choose_tobemodified_files(available_files: dict) -> dict:
    """é¸æ“‡è¦ä½¿ç”¨çš„ tobemodified æª”æ¡ˆï¼ˆæ”¯æ´å¤šé¸ï¼‰"""
    if not available_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• tobemodified æª”æ¡ˆ")
        return {}
    
    print("\nğŸ“„ å¯ç”¨çš„ tobemodified æª”æ¡ˆï¼š")
    choices = list(available_files.items())
    
    for i, (language, file_path) in enumerate(choices, 1):
        print(f"  {i}) {language} ({file_path.name})")
    
    print(f"  A) å…¨éƒ¨é¸æ“‡")
    print(f"  0) å–æ¶ˆæ“ä½œ")
    
    selected_files = {}
    
    while True:
        try:
            choice = input(f"\nè«‹é¸æ“‡è¦ä½¿ç”¨çš„æª”æ¡ˆ (å¯å¤šé¸ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œå¦‚ 1,2,3 æˆ– A)ï¼š").strip()
            
            if choice == '0':
                print("âŒ æ“ä½œå–æ¶ˆ")
                return {}
            elif choice.upper() == 'A':
                selected_files = available_files.copy()
                break
            else:
                # è§£æå¤šé¸
                choice_indices = [int(x.strip()) - 1 for x in choice.split(',')]
                selected_files = {}
                
                for choice_idx in choice_indices:
                    if 0 <= choice_idx < len(choices):
                        language, file_path = choices[choice_idx]
                        selected_files[language] = file_path
                    else:
                        print(f"âš ï¸  ç„¡æ•ˆé¸é …ï¼š{choice_idx + 1}")
                        continue
                
                if selected_files:
                    break
                else:
                    print(f"âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆçš„é¸é …")
                    
        except (ValueError, KeyboardInterrupt):
            print("\nâŒ æ“ä½œå–æ¶ˆ")
            return {}
    
    print(f"âœ… é¸æ“‡äº† {len(selected_files)} å€‹æª”æ¡ˆï¼š")
    for language, file_path in selected_files.items():
        print(f"   {language}: {file_path.name}")
    
    return selected_files


def choose_combine_file(files: list, file_type: str) -> Path:
    """é¸æ“‡è¦åˆä½µçš„æª”æ¡ˆ"""
    if not files:
        print(f"âš ï¸  /i18n_combine/ ä¸­æ²’æœ‰æ‰¾åˆ° {file_type.upper()} æª”æ¡ˆ")
        if file_type.lower() == 'po':
            print(f"ğŸ’¡ å°‡è‡ªå‹•å‰µå»ºé è¨­çš„ messages.po æª”æ¡ˆ")
        elif file_type.lower() == 'json':
            print(f"ğŸ’¡ å°‡è‡ªå‹•å‰µå»ºé è¨­çš„å¤šèªè¨€ JSON æª”æ¡ˆ")
        return None
    
    print(f"\nğŸ“ å¯ç”¨çš„ {file_type.upper()} æª”æ¡ˆï¼š")
    for i, file_info in enumerate(files, 1):
        print(f"  {i}) {file_info['relative_path']}")
    
    print(f"  0) è·³é {file_type.upper()} æª”æ¡ˆ")
    if file_type.lower() in ['po', 'json']:
        create_option = "messages.po" if file_type.lower() == 'po' else "å¤šèªè¨€ JSON"
        print(f"  C) å‰µå»ºæ–°çš„ {create_option} æª”æ¡ˆ")
    
    while True:
        try:
            choice = input(f"\nè«‹é¸æ“‡è¦åˆä½µçš„ {file_type.upper()} æª”æ¡ˆ (0-{len(files)}{'/C' if file_type.lower() in ['po', 'json'] else ''})ï¼š").strip()
            
            if choice == '0':
                print(f"â­ï¸  è·³é {file_type.upper()} æª”æ¡ˆ")
                return None
            elif choice.upper() == 'C' and file_type.lower() in ['po', 'json']:
                create_option = "messages.po" if file_type.lower() == 'po' else "å¤šèªè¨€ JSON"
                print(f"ğŸ†• å°‡å‰µå»ºæ–°çš„ {create_option} æª”æ¡ˆ")
                return "CREATE_NEW"
            else:
                choice_idx = int(choice)
                if 1 <= choice_idx <= len(files):
                    selected_file = files[choice_idx - 1]
                    print(f"âœ… é¸æ“‡äº†ï¼š{selected_file['relative_path']}")
                    return selected_file['path']
                else:
                    suffix = ' æˆ– C' if file_type.lower() in ['po', 'json'] else ''
                    print(f"âš ï¸  è«‹è¼¸å…¥ 0-{len(files)} ä¹‹é–“çš„æ•¸å­—{suffix}")
        except (ValueError, KeyboardInterrupt):
            print("\nâŒ æ“ä½œå–æ¶ˆ")
            return None


def read_excel_updates_for_language(xlsx_path: Path, language: str, config) -> dict:
    """è®€å–å–®å€‹èªè¨€çš„ Excel æª”æ¡ˆä¸­çš„æ›´æ–°è³‡æ–™"""
    try:
        print(f"ğŸ“– è®€å– {language} çš„ Excel æª”æ¡ˆï¼š{xlsx_path.name}")
        wb = openpyxl.load_workbook(xlsx_path, data_only=True)
        ws = wb.active
        
        header_row = list(ws[1])
        header = {cell.value: idx for idx, cell in enumerate(header_row) if cell.value}
        
        # åŸºæœ¬æ¬„ä½æª¢æŸ¥
        required_columns = ["æª”æ¡ˆé¡å‹", "é …ç›®ID", "é …ç›®å…§å®¹"]
        missing_columns = []
        
        for col in required_columns:
            if col not in header:
                missing_columns.append(col)
        
        if missing_columns:
            print(f"âŒ {language} Excel ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}")
            return {}
        
        # è‡ªå‹•æª¢æ¸¬æ‰€æœ‰æ¥­æ…‹çš„æ›¿æ›çµæœæ¬„ä½
        business_types = config.get_business_types()
        available_business_types = []
        
        for bt_code, bt_config in business_types.items():
            display_name = bt_config['display_name']
            result_col_name = f"{display_name}_æ›¿æ›çµæœ"
            if result_col_name in header:
                available_business_types.append(bt_code)
        
        if not available_business_types:
            print(f"âŒ {language} æœªæ‰¾åˆ°ä»»ä½•æ¥­æ…‹çš„æ›¿æ›çµæœæ¬„ä½")
            return {}
        
        print(f"   ğŸ“‹ {language} æª¢æ¸¬åˆ°æ¥­æ…‹ï¼š{', '.join([business_types[bt]['display_name'] for bt in available_business_types])}")
        
        # è§£ææ›´æ–°è³‡æ–™
        updates = {bt_code: {"po": [], "json": []} for bt_code in available_business_types}
        
        for row_num, row in enumerate(ws.iter_rows(min_row=2, values_only=True), start=2):
            if not row or len(row) <= max(header.values()):
                continue
            
            try:
                file_type = row[header["æª”æ¡ˆé¡å‹"]]
                entry_id = row[header["é …ç›®ID"]]
                original_text = row[header["é …ç›®å…§å®¹"]]
                
                if not file_type or not entry_id:
                    continue
                
                file_type = str(file_type).lower()
                
                # è™•ç†æ¯å€‹å¯ç”¨çš„æ¥­æ…‹
                for bt_code in available_business_types:
                    display_name = business_types[bt_code]['display_name']
                    result_col_name = f"{display_name}_æ›¿æ›çµæœ"
                    
                    new_value = row[header[result_col_name]]
                    
                    # è·³éç©ºå€¼å’Œèˆ‡åŸæ–‡ç›¸åŒçš„å€¼
                    if not new_value or not str(new_value).strip():
                        continue
                    
                    new_value = str(new_value).strip()
                    
                    if original_text and str(original_text).strip() == new_value:
                        continue
                    
                    # å‰µå»ºæ›´æ–°è¨˜éŒ„ï¼ŒåŒ…å«èªè¨€ä¿¡æ¯
                    update_record = (str(entry_id), new_value, language)
                    
                    if file_type == "po":
                        updates[bt_code]["po"].append(update_record)
                    elif file_type == "json":
                        updates[bt_code]["json"].append(update_record)
            
            except Exception as e:
                print(f"âš ï¸  {language} ç¬¬ {row_num} è¡Œè™•ç†å¤±æ•—: {e}")
                continue
        
        # çµ±è¨ˆæœ‰æ•ˆæ›´æ–°
        total_updates = 0
        for bt_code in available_business_types:
            bt_updates = len(updates[bt_code]["po"]) + len(updates[bt_code]["json"])
            total_updates += bt_updates
            if bt_updates > 0:
                print(f"     {business_types[bt_code]['display_name']}: {bt_updates} å€‹æ›´æ–°")
        
        print(f"   ğŸ“Š {language} ç¸½è¨ˆï¼š{total_updates} å€‹æœ‰æ•ˆæ›´æ–°")
        return updates
        
    except Exception as e:
        print(f"âŒ è®€å– {language} Excel æª”æ¡ˆå¤±æ•—ï¼š{e}")
        return {}


def combine_multilang_json_files_for_business_type(all_updates: dict, target_json_path: Path, 
                                                  output_json_path: Path, bt_code: str, log_detail=None,
                                                  create_new: bool = False, detected_languages: list = None) -> dict:
    """ã€v1.6 å¢å¼·ç‰ˆã€‘ç‚ºç‰¹å®šæ¥­æ…‹åˆä½µå¤šèªè¨€ JSON æª”æ¡ˆï¼Œæ”¯æ´å®Œæ•´é™£åˆ—æ›´æ–°"""
    result = {
        "success": False,
        "merged": 0,
        "skipped": 0,
        "conflicts": [],
        "errors": [],
        "language_stats": {},
        "created_new": False
    }
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç•¶å‰æ¥­æ…‹çš„æ›´æ–°
    json_updates_for_bt = []
    for language_updates in all_updates.values():
        if bt_code in language_updates and language_updates[bt_code]['json']:
            json_updates_for_bt.extend(language_updates[bt_code]['json'])
    
    if not json_updates_for_bt and not create_new:
        result["success"] = True
        if log_detail:
            log_detail(f"JSON ({bt_code}): æ²’æœ‰ä»»ä½•æ›´æ–°é …ç›®")
        return result
    
    try:
        # è™•ç†ç›®æ¨™ JSON æª”æ¡ˆ
        is_creating_new_file = False  # æ–°å¢æ¨™è¨˜è®Šæ•¸
        
        if create_new or target_json_path == "CREATE_NEW" or not target_json_path or not target_json_path.exists():
            # å‰µå»ºæ–°çš„ JSON æª”æ¡ˆ
            print(f"   ğŸ†• å‰µå»ºæ–°çš„å¤šèªè¨€ JSON æª”æ¡ˆï¼š{output_json_path.name}")
            if log_detail:
                log_detail(f"å‰µå»ºæ–°çš„å¤šèªè¨€ JSON æª”æ¡ˆï¼š{output_json_path.name}")
            
            # å‰µå»ºé è¨­æª”æ¡ˆåˆ°è‡¨æ™‚ä½ç½®
            temp_json_path = output_json_path.parent / f"temp_multilang.json"
            temp_json_path.parent.mkdir(parents=True, exist_ok=True)
            
            if not create_default_json_file(temp_json_path, all_updates, detected_languages or []):
                result["errors"].append(f"ç„¡æ³•å‰µå»ºé è¨­ JSON æª”æ¡ˆ")
                return result
            
            target_data = json.loads(temp_json_path.read_text(encoding="utf-8"))
            result["created_new"] = True
            is_creating_new_file = True  # è¨­ç½®ç‚ºæ–°å»ºæª”æ¡ˆæ¨™è¨˜
            
        else:
            # è¼‰å…¥ç¾æœ‰çš„ JSON æª”æ¡ˆ
            target_data = json.loads(target_json_path.read_text(encoding="utf-8"))
            print(f"   ğŸ“„ è¼‰å…¥ç›®æ¨™å¤šèªè¨€ JSON æª”æ¡ˆï¼š{target_json_path.name}")
            if log_detail:
                log_detail(f"è¼‰å…¥ç›®æ¨™ JSON æª”æ¡ˆï¼š{target_json_path.name}")
            is_creating_new_file = False  # æ˜ç¢ºè¨­ç½®ç‚ºéæ–°å»ºæª”æ¡ˆ
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå¤šèªè¨€çµæ§‹
        is_multilang_structure = check_multilang_json_structure(target_data)
        print(f"   ğŸ” å¤šèªè¨€çµæ§‹æª¢æ¸¬ï¼š{'æ˜¯' if is_multilang_structure else 'å¦'}")
        if log_detail:
            log_detail(f"å¤šèªè¨€çµæ§‹æª¢æ¸¬ï¼š{'æ˜¯' if is_multilang_structure else 'å¦'}")
        
        # è¼‰å…¥æ‰€æœ‰èªè¨€çš„åŸå§‹è³‡æ–™ç”¨æ–¼é™£åˆ—æ›´æ–°
        original_language_data = {}
        for language in all_updates.keys():
            original_data = load_original_language_json(language)
            if original_data:
                original_language_data[language] = original_data
                if log_detail:
                    log_detail(f"è¼‰å…¥ {language} åŸå§‹è³‡æ–™ç”¨æ–¼é™£åˆ—æ›´æ–°")
        
        conflicts = []
        language_stats = {}
        
        # åªè™•ç†ç•¶å‰æ¥­æ…‹çš„æ›´æ–°
        for language, language_updates in all_updates.items():
            if bt_code not in language_updates:
                continue
                
            language_stats[language] = {"merged": 0, "skipped": 0, "conflicts": 0}
            
            if log_detail:
                log_detail(f"è™•ç†èªè¨€ {language} çš„ JSON æ›´æ–° (æ¥­æ…‹: {bt_code})")
            
            # è™•ç†ç•¶å‰æ¥­æ…‹çš„ JSON æ›´æ–°
            bt_updates = language_updates[bt_code]
            for json_path_str, new_value, update_language in bt_updates['json']:
                if log_detail:
                    log_detail(f"è™•ç†æ›´æ–°ï¼š{update_language}.{json_path_str} = {new_value}")
                
                # æª¢æ¸¬æ˜¯å¦ç‚ºé™£åˆ—ç´¢å¼•è·¯å¾‘
                array_path, array_index = detect_array_path_and_index(json_path_str)
                
                if array_path is not None and array_index is not None:
                    # é€™æ˜¯é™£åˆ—ç´¢å¼•æ›´æ–°ï¼Œéœ€è¦é€²è¡Œå®Œæ•´é™£åˆ—æ›´æ–°
                    if log_detail:
                        log_detail(f"æª¢æ¸¬åˆ°é™£åˆ—ç´¢å¼•æ›´æ–°ï¼š{array_path}[{array_index}] = {new_value}")
                    
                    # å¾åŸå§‹èªè¨€è³‡æ–™ä¸­ç²å–å®Œæ•´é™£åˆ—
                    if update_language in original_language_data:
                        original_array = get_array_from_original_json(original_language_data[update_language], array_path)
                        
                        if original_array:
                            # ç¢ºä¿é™£åˆ—è¶³å¤ é•·
                            while len(original_array) <= array_index:
                                original_array.append("")
                            
                            # æ›´æ–°æŒ‡å®šç´¢å¼•çš„å€¼
                            original_array[array_index] = new_value
                            
                            # å¤šèªè¨€çµæ§‹çš„è·¯å¾‘æ˜ å°„
                            if is_multilang_structure:
                                final_path = f"{update_language}.{array_path}"
                            else:
                                final_path = array_path
                            
                            # è¨­ç½®å®Œæ•´é™£åˆ—åˆ°ç›®æ¨™è·¯å¾‘
                            if set_json_value_by_path(target_data, final_path, original_array):
                                result["merged"] += 1
                                language_stats[update_language]["merged"] += 1
                                if log_detail:
                                    log_detail(f"å®Œæ•´é™£åˆ—æ›´æ–°æˆåŠŸï¼š{final_path} = {original_array}")
                            else:
                                error_msg = f"ç„¡æ³•è¨­ç½®å®Œæ•´é™£åˆ—ï¼š{final_path}"
                                result["errors"].append(error_msg)
                                if log_detail:
                                    log_detail(f"éŒ¯èª¤ï¼š{error_msg}")
                        else:
                            # ç„¡æ³•ç²å–åŸå§‹é™£åˆ—ï¼Œä½¿ç”¨å‚³çµ±æ–¹å¼
                            if log_detail:
                                log_detail(f"ç„¡æ³•ç²å–åŸå§‹é™£åˆ—ï¼Œä½¿ç”¨å‚³çµ±ç´¢å¼•æ›´æ–°ï¼š{json_path_str}")
                            
                            # å¤šèªè¨€çµæ§‹çš„è·¯å¾‘æ˜ å°„
                            if is_multilang_structure:
                                multilang_path = f"{update_language}.{json_path_str}"
                            else:
                                multilang_path = json_path_str
                            
                            # å‚³çµ±çš„ç´¢å¼•æ›´æ–°æ–¹å¼
                            if set_json_value_by_path(target_data, multilang_path, new_value):
                                result["merged"] += 1
                                language_stats[update_language]["merged"] += 1
                                if log_detail:
                                    log_detail(f"å‚³çµ±ç´¢å¼•æ›´æ–°æˆåŠŸï¼š{multilang_path} = {new_value}")
                            else:
                                error_msg = f"ç„¡æ³•è¨­ç½®å‚³çµ±ç´¢å¼•è·¯å¾‘ï¼š{multilang_path}"
                                result["errors"].append(error_msg)
                                if log_detail:
                                    log_detail(f"éŒ¯èª¤ï¼š{error_msg}")
                    else:
                        if log_detail:
                            log_detail(f"æœªæ‰¾åˆ° {update_language} çš„åŸå§‹è³‡æ–™ï¼Œä½¿ç”¨å‚³çµ±æ›´æ–°æ–¹å¼")
                        
                        # å¤šèªè¨€çµæ§‹çš„è·¯å¾‘æ˜ å°„
                        if is_multilang_structure:
                            multilang_path = f"{update_language}.{json_path_str}"
                        else:
                            multilang_path = json_path_str
                        
                        # å‚³çµ±çš„ç´¢å¼•æ›´æ–°æ–¹å¼
                        if set_json_value_by_path(target_data, multilang_path, new_value):
                            result["merged"] += 1
                            language_stats[update_language]["merged"] += 1
                            if log_detail:
                                log_detail(f"å‚³çµ±ç´¢å¼•æ›´æ–°æˆåŠŸï¼š{multilang_path} = {new_value}")
                        else:
                            error_msg = f"ç„¡æ³•è¨­ç½®å‚³çµ±ç´¢å¼•è·¯å¾‘ï¼š{multilang_path}"
                            result["errors"].append(error_msg)
                            if log_detail:
                                log_detail(f"éŒ¯èª¤ï¼š{error_msg}")
                
                else:
                    # é€™æ˜¯æ™®é€šè·¯å¾‘æ›´æ–°ï¼ˆéé™£åˆ—ç´¢å¼•ï¼‰
                    # å¤šèªè¨€çµæ§‹çš„è·¯å¾‘æ˜ å°„
                    if is_multilang_structure:
                        multilang_path = f"{update_language}.{json_path_str}"
                    else:
                        multilang_path = json_path_str
                    
                    # ç²å–ç¾æœ‰å€¼
                    existing_value = get_json_value_by_path(target_data, multilang_path)
                    
                    # ä¿®æ­£çš„è¡çªæª¢æ¸¬é‚è¼¯ï¼šæ–°å»ºæª”æ¡ˆæ™‚è·³éè¡çªæª¢æ¸¬
                    if not is_creating_new_file and existing_value is not None:
                        existing_str = str(existing_value).strip()
                        new_str = str(new_value).strip()
                        
                        # å¦‚æœå€¼å®Œå…¨ç›¸åŒï¼Œè·³é
                        if existing_str == new_str:
                            result["skipped"] += 1
                            language_stats[update_language]["skipped"] += 1
                            if log_detail:
                                log_detail(f"è·³éç›¸åŒå€¼ï¼š{multilang_path} = '{new_str}'")
                            continue
                        
                        # ç•¶å€¼ä¸åŒä¸”ä¸æ˜¯ç©ºå­—ä¸²æ™‚ï¼Œæ¨™è¨˜ç‚ºè¡çªä¸¦è®“ç”¨æˆ¶æ±ºå®š
                        if existing_str != new_str and existing_str != "":
                            conflict_info = {
                                "path": multilang_path,
                                "language": update_language,
                                "existing_value": existing_str,
                                "new_value": new_str,
                                "file_type": "json"
                            }
                            conflicts.append(conflict_info)
                            result["conflicts"].append(conflict_info)
                            language_stats[update_language]["conflicts"] += 1
                            
                            if log_detail:
                                log_detail(f"ç™¼ç¾è¡çªï¼š{multilang_path}")
                                log_detail(f"  ç¾æœ‰å€¼: '{existing_str}'")
                                log_detail(f"  æ–°å€¼: '{new_str}'")
                            
                            # è©¢å•ç”¨æˆ¶å¦‚ä½•è™•ç†è¡çª
                            choice = handle_json_conflict(multilang_path, existing_str, new_str, update_language)
                            
                            if choice == "keep_existing":
                                result["skipped"] += 1
                                language_stats[update_language]["skipped"] += 1
                                if log_detail:
                                    log_detail(f"ä¿ç•™ç¾æœ‰å€¼ï¼š{multilang_path} = '{existing_str}'")
                                continue
                            elif choice == "use_new":
                                # ç¹¼çºŒåŸ·è¡Œæ›´æ–°é‚è¼¯
                                if log_detail:
                                    log_detail(f"æ¡ç”¨æ–°å€¼ï¼š{multilang_path} = '{new_str}'")
                            elif choice == "skip":
                                result["skipped"] += 1
                                language_stats[update_language]["skipped"] += 1
                                if log_detail:
                                    log_detail(f"è·³éè™•ç†ï¼š{multilang_path}")
                                continue
                    
                    # æ‡‰ç”¨æ™®é€šæ›´æ–°
                    if set_json_value_by_path(target_data, multilang_path, new_value):
                        result["merged"] += 1
                        language_stats[update_language]["merged"] += 1
                        if log_detail:
                            if is_creating_new_file:
                                log_detail(f"æ–°å»ºæª”æ¡ˆå¯«å…¥ï¼š{multilang_path} = '{new_value}'")
                            else:
                                original_display = f"'{existing_value}'" if existing_value is not None else "ç„¡"
                                log_detail(f"æˆåŠŸæ›´æ–°ï¼š{multilang_path} = '{new_value}' (åŸå€¼: {original_display})")
                    else:
                        error_msg = f"ç„¡æ³•è¨­ç½® JSON è·¯å¾‘ï¼š{multilang_path} (èªè¨€: {update_language})"
                        result["errors"].append(error_msg)
                        if log_detail:
                            log_detail(f"éŒ¯èª¤ï¼š{error_msg}")
        
        # ä¿å­˜åˆä½µå¾Œçš„æª”æ¡ˆ
        output_json_path.parent.mkdir(parents=True, exist_ok=True)
        
        json_content = json.dumps(target_data, ensure_ascii=False, indent=2)
        output_json_path.write_text(json_content, encoding="utf-8")
        
        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        temp_json_path = output_json_path.parent / f"temp_multilang.json"
        if temp_json_path.exists():
            temp_json_path.unlink()
        
        result["success"] = True
        result["language_stats"] = language_stats
        
        # ä¿®æ­£æ—¥èªŒè¨Šæ¯ï¼ŒåŒ…å«è¡çªæ•¸é‡
        total_conflicts = len(conflicts)
        if log_detail:
            status = "å‰µå»ºä¸¦" if result["created_new"] else ""
            log_detail(f"JSON ({bt_code}) {status}åˆä½µå®Œæˆï¼šåˆä½µ {result['merged']} å€‹ï¼Œè·³é {result['skipped']} å€‹ï¼Œè¡çª {total_conflicts} å€‹")
        
    except json.JSONDecodeError as e:
        error_msg = f"JSON æ ¼å¼éŒ¯èª¤ï¼š{e}"
        result["errors"].append(error_msg)
        if log_detail:
            log_detail(f"éŒ¯èª¤ï¼š{error_msg}")
    except Exception as e:
        error_msg = f"JSON æª”æ¡ˆåˆä½µå¤±æ•—ï¼š{e}"
        result["errors"].append(error_msg)
        if log_detail:
            log_detail(f"éŒ¯èª¤ï¼š{error_msg}")
    
    return result


def handle_json_conflict(path: str, existing_value: str, new_value: str, language: str) -> str:
    """è™•ç† JSON åˆä½µè¡çªï¼Œè®“ç”¨æˆ¶é¸æ“‡å¦‚ä½•è™•ç†"""
    print(f"\nâš ï¸  ç™¼ç¾è¡çªï¼š")
    print(f"ğŸ“ è·¯å¾‘ï¼š{path}")
    print(f"ğŸŒ èªè¨€ï¼š{language}")
    print(f"ğŸ“„ ç¾æœ‰å€¼ï¼š'{existing_value}'")
    print(f"ğŸ†• æ–°å€¼ï¼š'{new_value}'")
    
    while True:
        print(f"\nè«‹é¸æ“‡è™•ç†æ–¹å¼ï¼š")
        print(f"  1) ä¿ç•™ç¾æœ‰å€¼ ('{existing_value}')")
        print(f"  2) ä½¿ç”¨æ–°å€¼ ('{new_value}')")
        print(f"  3) è·³éæ­¤é …ç›®")
        print(f"  A) å°æ‰€æœ‰é¡ä¼¼è¡çªä½¿ç”¨æ–°å€¼")
        print(f"  K) å°æ‰€æœ‰é¡ä¼¼è¡çªä¿ç•™ç¾æœ‰å€¼")
        
        try:
            choice = input(f"è«‹é¸æ“‡ (1/2/3/A/K)ï¼š").strip().upper()
            
            if choice == "1":
                return "keep_existing"
            elif choice == "2":
                return "use_new"
            elif choice == "3":
                return "skip"
            elif choice == "A":
                print(f"âœ… å°‡ä½¿ç”¨æ–°å€¼")
                return "use_new"
            elif choice == "K":
                print(f"âœ… å°‡ä¿ç•™ç¾æœ‰å€¼")
                return "keep_existing"
            else:
                print(f"âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆé¸é … (1/2/3/A/K)")
                
        except KeyboardInterrupt:
            print(f"\nâŒ æ“ä½œå–æ¶ˆï¼Œè·³éæ­¤é …ç›®")
            return "skip"


def generate_conflict_report(conflicts: list, output_dir: Path, timestamp: str):
    """ç”Ÿæˆè¡çªå ±å‘Š"""
    if not conflicts:
        return
    
    conflict_report_file = output_dir / f"conflicts_report_{timestamp}.txt"
    
    try:
        with open(conflict_report_file, 'w', encoding='utf-8') as f:
            f.write(f"JSON åˆä½µè¡çªå ±å‘Š\n")
            f.write(f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*60}\n\n")
            
            f.write(f"ç¸½è¡çªæ•¸é‡ï¼š{len(conflicts)}\n\n")
            
            for i, conflict in enumerate(conflicts, 1):
                f.write(f"è¡çª {i}ï¼š\n")
                f.write(f"  è·¯å¾‘ï¼š{conflict['path']}\n")
                f.write(f"  èªè¨€ï¼š{conflict['language']}\n")
                f.write(f"  ç¾æœ‰å€¼ï¼š'{conflict['existing_value']}'\n")
                f.write(f"  æ–°å€¼ï¼š'{conflict['new_value']}'\n")
                f.write(f"  æª”æ¡ˆé¡å‹ï¼š{conflict['file_type']}\n")
                f.write(f"\n{'-'*40}\n\n")
            
            f.write(f"è™•ç†å»ºè­°ï¼š\n")
            f.write(f"1. æª¢æŸ¥å€¼çš„å·®ç•°æ˜¯å¦ç‚ºé æœŸçš„æ›´æ–°\n")
            f.write(f"2. ç¢ºèªèªè¨€ç¿»è­¯çš„æ­£ç¢ºæ€§\n")
            f.write(f"3. é©—è­‰æ¥­æ…‹ç‰¹å®šçš„è¡“èªä½¿ç”¨\n")
            f.write(f"4. è€ƒæ…®å»ºç«‹ç¿»è­¯ä¸€è‡´æ€§æª¢æŸ¥æ©Ÿåˆ¶\n")
        
        print(f"ğŸ“„ è¡çªå ±å‘Šå·²ç”Ÿæˆï¼š{conflict_report_file}")
        
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆè¡çªå ±å‘Šå¤±æ•—ï¼š{e}")


def combine_po_files_for_business_type(all_updates: dict, target_po_path: Path, 
                                     output_dir: Path, bt_code: str, log_detail=None, 
                                     create_new: bool = False) -> dict:
    """ã€å¢å¼·ç‰ˆã€‘ç‚ºç‰¹å®šæ¥­æ…‹è™•ç† PO æª”æ¡ˆåˆä½µï¼Œæ¯å€‹èªè¨€ç”Ÿæˆç¨ç«‹çš„ PO æª”æ¡ˆ"""
    result = {
        "success": False,
        "merged": 0,
        "skipped": 0,
        "conflicts": [],
        "errors": [],
        "language_stats": {},
        "created_new": False,
        "created_files": []  # æ–°å¢ï¼šè¨˜éŒ„å‰µå»ºçš„æª”æ¡ˆ
    }
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç•¶å‰æ¥­æ…‹çš„ PO æ›´æ–°
    languages_with_po_updates = {}
    for language, language_updates in all_updates.items():
        if bt_code in language_updates and language_updates[bt_code]['po']:
            languages_with_po_updates[language] = language_updates[bt_code]['po']
    
    if not languages_with_po_updates:
        result["success"] = True
        if log_detail:
            log_detail(f"PO ({bt_code}): æ²’æœ‰ä»»ä½•æ›´æ–°é …ç›®")
        return result
    
    try:
        config = get_config()
        business_types = config.get_business_types()
        suffix = business_types[bt_code]['suffix'] if bt_code in business_types else ""
        
        # ç‚ºæ¯å€‹èªè¨€åˆ†åˆ¥è™•ç† PO æª”æ¡ˆ
        for language, po_updates in languages_with_po_updates.items():
            print(f"   ğŸŒ è™•ç† {language} çš„ PO æª”æ¡ˆ...")
            if log_detail:
                log_detail(f"é–‹å§‹è™•ç† {language} çš„ PO æª”æ¡ˆ (æ¥­æ…‹: {bt_code})")
            
            # ç¢ºå®šç•¶å‰èªè¨€çš„è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            if target_po_path and target_po_path != "CREATE_NEW":
                # åŸºæ–¼åŸå§‹æª”æ¡ˆåç¨±ï¼Œæ·»åŠ èªè¨€å’Œæ¥­æ…‹å¾Œç¶´
                base_name = target_po_path.stem
                # ç§»é™¤å¯èƒ½å·²å­˜åœ¨çš„èªè¨€å¾Œç¶´ï¼Œé¿å…é‡è¤‡
                if base_name.endswith(f"_{language}"):
                    base_name = base_name[:-len(f"_{language}")]
                output_po_path = output_dir / f"{base_name}_{language}{suffix}_combined.po"
            else:
                output_po_path = output_dir / f"messages_{language}{suffix}_combined.po"
            
            # è¨˜éŒ„å‰µå»ºçš„æª”æ¡ˆ
            result["created_files"].append(str(output_po_path))
            
            # ç‚ºç•¶å‰èªè¨€å‰µå»ºæˆ–è¼‰å…¥ PO æª”æ¡ˆ
            if create_new or target_po_path == "CREATE_NEW" or not target_po_path or not target_po_path.exists():
                # å‰µå»ºæ–°çš„ PO æª”æ¡ˆ
                print(f"     ğŸ†• å‰µå»ºæ–°çš„ PO æª”æ¡ˆï¼š{output_po_path.name}")
                if log_detail:
                    log_detail(f"å‰µå»ºæ–°çš„ PO æª”æ¡ˆï¼š{output_po_path.name}")
                
                # å‰µå»ºé è¨­æª”æ¡ˆ
                if not create_default_po_file(output_po_path, language):
                    result["errors"].append(f"ç„¡æ³•ç‚º {language} å‰µå»ºé è¨­ PO æª”æ¡ˆ")
                    continue
                
                target_po = polib.pofile(str(output_po_path))
                result["created_new"] = True
                
                # æ¸…ç©ºé è¨­æ¢ç›®ï¼Œå°‡ç”±æ›´æ–°è³‡æ–™å¡«å……
                target_po.clear()
                
            else:
                # å˜—è©¦è¼‰å…¥å°æ‡‰èªè¨€çš„ç¾æœ‰ PO æª”æ¡ˆ
                language_specific_path = target_po_path.parent / f"{target_po_path.stem}_{language}.po"
                if language_specific_path.exists():
                    target_po = polib.pofile(str(language_specific_path))
                    print(f"     ğŸ“„ è¼‰å…¥ {language} å°ˆç”¨ PO æª”æ¡ˆï¼š{language_specific_path.name}")
                    if log_detail:
                        log_detail(f"è¼‰å…¥ {language} å°ˆç”¨ PO æª”æ¡ˆï¼š{language_specific_path.name}")
                else:
                    # ä½¿ç”¨é€šç”¨ PO æª”æ¡ˆä½œç‚ºåŸºç¤
                    target_po = polib.pofile(str(target_po_path))
                    print(f"     ğŸ“„ åŸºæ–¼é€šç”¨ PO æª”æ¡ˆå‰µå»º {language} ç‰ˆæœ¬")
                    if log_detail:
                        log_detail(f"åŸºæ–¼é€šç”¨ PO æª”æ¡ˆå‰µå»º {language} ç‰ˆæœ¬")
            
            # åˆå§‹åŒ–ç•¶å‰èªè¨€çš„çµ±è¨ˆ
            language_stats = {"merged": 0, "skipped": 0, "conflicts": 0}
            
            # è™•ç†ç•¶å‰èªè¨€çš„ PO æ›´æ–°
            for msgid, new_msgstr, update_language in po_updates:
                target_entry = target_po.find(msgid)
                
                if target_entry:
                    # åªæœ‰ç•¶ç¾æœ‰å€¼å’Œæ–°å€¼çœŸçš„ä¸åŒæ™‚æ‰éœ€è¦æ›´æ–°
                    if target_entry.msgstr and target_entry.msgstr.strip():
                        if target_entry.msgstr == new_msgstr:
                            # å€¼ç›¸åŒï¼Œè·³é
                            language_stats["skipped"] += 1
                            result["skipped"] += 1
                            if log_detail:
                                log_detail(f"[{language}] è·³éç›¸åŒå€¼ï¼š{msgid} = '{new_msgstr}'")
                            continue
                        else:
                            # å€¼ä¸åŒï¼Œè¨˜éŒ„ä½†ä»ç„¶æ›´æ–°
                            if log_detail:
                                log_detail(f"[{language}] æ›´æ–°ç¾æœ‰æ¢ç›®ï¼š{msgid} = '{new_msgstr}' (åŸå€¼: '{target_entry.msgstr}')")
                    
                    # æ‡‰ç”¨æ›´æ–°
                    target_entry.msgstr = new_msgstr
                    language_stats["merged"] += 1
                    result["merged"] += 1
                    
                else:
                    # ç›®æ¨™æª”æ¡ˆä¸­æ²’æœ‰æ­¤æ¢ç›®ï¼Œæ·»åŠ æ–°æ¢ç›®
                    new_entry = polib.POEntry(
                        msgid=msgid,
                        msgstr=new_msgstr
                    )
                    target_po.append(new_entry)
                    language_stats["merged"] += 1
                    result["merged"] += 1
                    if log_detail:
                        log_detail(f"[{language}] æ–°å¢æ¢ç›®ï¼š{msgid} = '{new_msgstr}'")
            
            # æ›´æ–° PO æª”æ¡ˆçš„èªè¨€å…ƒæ•¸æ“š
            if 'Language' in target_po.metadata:
                target_po.metadata['Language'] = language
            if 'Language-Team' in target_po.metadata:
                target_po.metadata['Language-Team'] = f'{language} <LL@li.org>'
            
            # ä¿å­˜ç•¶å‰èªè¨€çš„ PO æª”æ¡ˆ
            output_po_path.parent.mkdir(parents=True, exist_ok=True)
            target_po.save(str(output_po_path))
            
            # è¨˜éŒ„èªè¨€çµ±è¨ˆ
            result["language_stats"][language] = language_stats
            
            print(f"     âœ… {language}: åˆä½µ {language_stats['merged']} å€‹ï¼Œè·³é {language_stats['skipped']} å€‹")
            if log_detail:
                log_detail(f"[{language}] PO æª”æ¡ˆè™•ç†å®Œæˆï¼šåˆä½µ {language_stats['merged']} å€‹ï¼Œè·³é {language_stats['skipped']} å€‹")
        
        result["success"] = True
        
        if log_detail:
            log_detail(f"PO ({bt_code}) è™•ç†å®Œæˆï¼šå…±è™•ç† {len(languages_with_po_updates)} å€‹èªè¨€")
            log_detail(f"ç¸½è¨ˆï¼šåˆä½µ {result['merged']} å€‹ï¼Œè·³é {result['skipped']} å€‹")
            log_detail(f"ç”Ÿæˆæª”æ¡ˆï¼š{', '.join(result['created_files'])}")
        
    except Exception as e:
        error_msg = f"PO æª”æ¡ˆåˆä½µå¤±æ•—ï¼š{e}"
        result["errors"].append(error_msg)
        if log_detail:
            log_detail(f"éŒ¯èª¤ï¼š{error_msg}")
    
    return result


def get_json_value_by_path(data: dict, path: str):
    """æŒ‰è·¯å¾‘ç²å– JSON å€¼"""
    try:
        path_parts = parse_json_path(path)
        current = data
        
        for part_type, part_value in path_parts:
            if part_type == 'key':
                if part_value not in current:
                    return None
                current = current[part_value]
            elif part_type == 'index':
                if not isinstance(current, list) or len(current) <= part_value:
                    return None
                current = current[part_value]
        
        return current
        
    except Exception:
        return None


def set_json_value_by_path(data: dict, path: str, new_value) -> bool:
    """ã€v1.6 å¢å¼·ç‰ˆã€‘æŒ‰è·¯å¾‘è¨­ç½® JSON å€¼ï¼Œæ”¯æ´é™£åˆ—å’Œæ™®é€šå€¼"""
    try:
        path_parts = parse_json_path(path)
        current = data
        
        for i, (part_type, part_value) in enumerate(path_parts):
            is_last = (i == len(path_parts) - 1)
            
            if part_type == 'key':
                if is_last:
                    current[part_value] = new_value
                else:
                    if part_value not in current:
                        next_part_type = path_parts[i + 1][0] if i + 1 < len(path_parts) else 'key'
                        current[part_value] = [] if next_part_type == 'index' else {}
                    current = current[part_value]
            
            elif part_type == 'index':
                if is_last:
                    while len(current) <= part_value:
                        current.append(None)
                    current[part_value] = new_value
                else:
                    while len(current) <= part_value:
                        current.append(None)
                    if current[part_value] is None:
                        next_part_type = path_parts[i + 1][0] if i + 1 < len(path_parts) else 'key'
                        current[part_value] = [] if next_part_type == 'index' else {}
                    current = current[part_value]
        
        return True
        
    except Exception as e:
        print(f"âš ï¸  è¨­ç½®JSONè·¯å¾‘å¤±æ•—ï¼š{path} = {new_value}, éŒ¯èª¤ï¼š{e}")
        return False


def parse_json_path(path: str) -> list:
    """è§£æ JSON è·¯å¾‘"""
    parts = []
    current = ""
    in_bracket = False
    
    for char in path:
        if char == '[':
            if current:
                parts.append(('key', current))
                current = ""
            in_bracket = True
        elif char == ']':
            if in_bracket and current:
                try:
                    parts.append(('index', int(current)))
                except ValueError:
                    raise ValueError(f"ç„¡æ•ˆçš„é™£åˆ—ç´¢å¼•ï¼š{current}")
                current = ""
            in_bracket = False
        elif char == '.' and not in_bracket:
            if current:
                parts.append(('key', current))
                current = ""
        else:
            current += char
    
    if current:
        parts.append(('key', current))
    
    return parts


def check_po_updates_exist(all_updates: dict) -> bool:
    """æª¢æŸ¥æ˜¯å¦å­˜åœ¨ä»»ä½• PO æ›´æ–°"""
    for language_updates in all_updates.values():
        for bt_code, bt_updates in language_updates.items():
            if bt_updates['po']:
                return True
    return False


def check_json_updates_exist(all_updates: dict) -> bool:
    """æª¢æŸ¥æ˜¯å¦å­˜åœ¨ä»»ä½• JSON æ›´æ–°"""
    for language_updates in all_updates.values():
        for bt_code, bt_updates in language_updates.items():
            if bt_updates['json']:
                return True
    return False


def generate_multilang_summary_report(results: dict, all_updates: dict, output_dir: Path, timestamp: str, 
                                     target_json_path: Path, target_po_path: Path, log_detail):
    """ç”Ÿæˆå¤šèªè¨€åˆä½µè™•ç†æ‘˜è¦å ±å‘Š"""
    summary_file = output_dir / f"multi_combine_summary_{timestamp}.txt"
    
    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"å¤šèªè¨€æª”æ¡ˆåˆä½µè™•ç†æ‘˜è¦å ±å‘Š\n")
            f.write(f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*60}\n\n")
            
            f.write(f"ç›®æ¨™æª”æ¡ˆï¼š\n")
            if target_json_path:
                if target_json_path == "CREATE_NEW":
                    f.write(f"  JSON: å‰µå»ºæ–°çš„å¤šèªè¨€ JSON\n")
                else:
                    f.write(f"  JSON: {target_json_path}\n")
            if target_po_path:
                if target_po_path == "CREATE_NEW":
                    f.write(f"  PO: å‰µå»ºæ–°çš„ messages.po\n")
                else:
                    f.write(f"  PO: {target_po_path}\n")
            f.write(f"\n")
            
            f.write(f"è™•ç†çš„èªè¨€ï¼š\n")
            for language in all_updates.keys():
                f.write(f"  - {language}\n")
            f.write(f"\n")
            
            total_merged = 0
            total_skipped = 0
            total_errors = 0
            successful_business_types = []
            failed_business_types = []
            created_new_files = []
            
            # æŒ‰æ¥­æ…‹çµ±è¨ˆ
            for bt_code, bt_results in results.items():
                f.write(f"æ¥­æ…‹ï¼š{bt_code}\n")
                
                bt_merged = sum(result.get('merged', 0) for result in bt_results.values())
                bt_skipped = sum(result.get('skipped', 0) for result in bt_results.values())
                bt_errors = []
                bt_new_files = []
                
                for result_key, result in bt_results.items():
                    bt_errors.extend(result.get('errors', []))
                    if result.get('created_new'):
                        file_type = "JSONæª”æ¡ˆ" if "json" in result_key else "POæª”æ¡ˆ"
                        bt_new_files.append(file_type)
                
                f.write(f"åˆä½µæ•¸é‡ï¼š{bt_merged}\n")
                f.write(f"è·³éæ•¸é‡ï¼š{bt_skipped}\n")
                
                if bt_new_files:
                    f.write(f"æ–°å»ºæª”æ¡ˆï¼š{', '.join(bt_new_files)}\n")
                    created_new_files.extend(bt_new_files)
                
                # èªè¨€ç´šåˆ¥çµ±è¨ˆ
                f.write(f"èªè¨€çµ±è¨ˆï¼š\n")
                for result in bt_results.values():
                    if 'language_stats' in result:
                        for lang, stats in result['language_stats'].items():
                            f.write(f"  {lang}: åˆä½µ {stats['merged']}, è·³é {stats['skipped']}, è¡çª {stats.get('conflicts', 0)}\n")
                
                if bt_errors:
                    f.write(f"éŒ¯èª¤ï¼š\n")
                    for error in bt_errors:
                        f.write(f"  - {error}\n")
                    failed_business_types.append(bt_code)
                else:
                    successful_business_types.append(bt_code)
                
                total_merged += bt_merged
                total_skipped += bt_skipped
                total_errors += len(bt_errors)
                
                f.write(f"\n{'-'*40}\n\n")
            
            # ç¸½è¨ˆçµ±è¨ˆ
            f.write(f"è™•ç†ç¸½çµï¼š\n")
            f.write(f"æˆåŠŸæ¥­æ…‹ï¼š{len(successful_business_types)}\n")
            f.write(f"å¤±æ•—æ¥­æ…‹ï¼š{len(failed_business_types)}\n")
            f.write(f"ç¸½åˆä½µé …ç›®ï¼š{total_merged}\n")
            f.write(f"ç¸½è·³éé …ç›®ï¼š{total_skipped}\n")
            f.write(f"ç¸½éŒ¯èª¤é …ç›®ï¼š{total_errors}\n")
            f.write(f"è™•ç†èªè¨€æ•¸ï¼š{len(all_updates)}\n")
            
            if created_new_files:
                f.write(f"æ–°å»ºæª”æ¡ˆæ•¸ï¼š{len(set(created_new_files))}\n")
            
            if successful_business_types:
                f.write(f"\næˆåŠŸçš„æ¥­æ…‹ï¼š{', '.join(successful_business_types)}\n")
            
            if failed_business_types:
                f.write(f"å¤±æ•—çš„æ¥­æ…‹ï¼š{', '.join(failed_business_types)}\n")
            
            # v1.6 ç‰ˆæœ¬æ–°å¢èªªæ˜
            f.write(f"\nå¤šèªè¨€åˆä½µèªªæ˜ï¼š\n")
            f.write(f"- æœ¬æ¬¡è™•ç†æ”¯æ´å¤šå€‹èªè¨€çš„ tobemodified åˆä½µ\n")
            f.write(f"- JSON æª”æ¡ˆï¼šæ¡ç”¨å¤šèªè¨€çµæ§‹ï¼Œæ‰€æœ‰èªè¨€åˆä½µåˆ°åŒä¸€æª”æ¡ˆ\n")
            f.write(f"- PO æª”æ¡ˆï¼šæ¯å€‹èªè¨€ç”Ÿæˆç¨ç«‹çš„ PO æª”æ¡ˆï¼ˆå¦‚ messages_zh_TW_rt.poï¼‰\n")
            f.write(f"- è‡ªå‹•æª¢æ¸¬ä¸¦è™•ç†èªè¨€å±¤ç´šçš„è·¯å¾‘æ˜ å°„\n")
            f.write(f"- æŒ‰æ¥­æ…‹åˆ†åˆ¥è™•ç†ï¼Œé¿å…æ¥­æ…‹é–“ç›¸äº’å¹²æ“¾\n")
            f.write(f"- ç›¸åŒ key ä¸”ç›¸åŒ value çš„é …ç›®æœƒè‡ªå‹•è·³é\n")
            f.write(f"- ä¸åŒ value çš„é …ç›®æœƒæ­£å¸¸æ›´æ–°\n")
            f.write(f"- æ²’æœ‰ç›®æ¨™æª”æ¡ˆæ™‚æœƒè‡ªå‹•å‰µå»ºæ¨™æº–æª”æ¡ˆï¼ˆJSON/POï¼‰\n")
            
            f.write(f"\nv1.6 ç‰ˆæœ¬æ–°å¢åŠŸèƒ½ - æ™ºèƒ½é™£åˆ—è™•ç†ï¼š\n")
            f.write(f"- æª¢æ¸¬é™£åˆ—ç´¢å¼•è·¯å¾‘ï¼ˆå¦‚ slogan[1]ï¼‰ä¸¦è‡ªå‹•é€²è¡Œå®Œæ•´é™£åˆ—æ›´æ–°\n")
            f.write(f"- å¾ i18n_input/{{language}}/{{language}}.json è®€å–åŸå§‹å®Œæ•´é™£åˆ—\n")
            f.write(f"- åªæ›¿æ›æŒ‡å®šç´¢å¼•çš„å…ƒç´ ï¼Œä¿æŒå…¶ä»–å…ƒç´ ä¸è®Š\n")
            f.write(f"- é¿å…é™£åˆ—éƒ¨åˆ†æ›´æ–°å°è‡´å…¶ä»–ä½ç½®è®Šæˆ null çš„å•é¡Œ\n")
            f.write(f"- æ”¯æ´åµŒå¥—é™£åˆ—è·¯å¾‘ï¼ˆå¦‚ data.items[0].tags[2]ï¼‰\n")
            f.write(f"- ç•¶ç„¡æ³•ç²å–åŸå§‹é™£åˆ—æ™‚ï¼Œè‡ªå‹•é™ç´šç‚ºå‚³çµ±ç´¢å¼•æ›´æ–°\n")
            f.write(f"- éé™£åˆ—ç´¢å¼•è·¯å¾‘ä»ä½¿ç”¨åŸæœ‰çš„æ›´æ–°é‚è¼¯\n")
            
            f.write(f"\nä½¿ç”¨å»ºè­°ï¼š\n")
            f.write(f"- ç¢ºèªç›®æ¨™ JSON æª”æ¡ˆæ¡ç”¨å¤šèªè¨€çµæ§‹ï¼ˆé ‚å±¤ç‚ºèªè¨€ä»£ç¢¼ï¼‰\n")
            f.write(f"- PO æª”æ¡ˆæœƒæŒ‰èªè¨€åˆ†åˆ¥ç”Ÿæˆï¼Œä¾¿æ–¼ç¨ç«‹ç®¡ç†å„èªè¨€ç¿»è­¯\n")
            f.write(f"- åˆä½µå‰å»ºè­°å‚™ä»½åŸå§‹æª”æ¡ˆ\n")
            f.write(f"- åˆä½µå¾Œè«‹æ¸¬è©¦ç¿»è­¯æª”æ¡ˆçš„æ­£ç¢ºæ€§\n")
            f.write(f"- æª¢æŸ¥å„èªè¨€æª”æ¡ˆçš„æ•¸æ“šå®Œæ•´æ€§\n")
            f.write(f"- æ–°å»ºçš„æª”æ¡ˆåŒ…å«æ¨™æº–çµæ§‹ï¼Œç„¡é è¨­ç¯„ä¾‹\n")
            f.write(f"- ç¢ºä¿ i18n_input ç›®éŒ„åŒ…å«å„èªè¨€çš„åŸå§‹ JSON æª”æ¡ˆä»¥æ”¯æ´é™£åˆ—æ›´æ–°\n")
            f.write(f"- é™£åˆ—ç´¢å¼•æ›´æ–°æœƒè‡ªå‹•å¾åŸå§‹æª”æ¡ˆè®€å–å®Œæ•´é™£åˆ—é€²è¡Œæ™ºèƒ½åˆä½µ\n")
            
        log_detail(f"å¤šèªè¨€åˆä½µæ‘˜è¦å ±å‘Šå·²ç”Ÿæˆï¼š{summary_file}")
        
    except Exception as e:
        log_detail(f"ç”Ÿæˆå¤šèªè¨€åˆä½µæ‘˜è¦å ±å‘Šå¤±æ•—ï¼š{e}")


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸš€ é–‹å§‹å¤šèªè¨€æª”æ¡ˆåˆä½µè™•ç† (v1.6 - æ”¯æ´é™£åˆ—å®Œæ•´æ›´æ–°ç‰ˆ)")
    
    # è¼‰å…¥é…ç½®
    config = get_config()
    
    # æª¢æ¸¬å¯ç”¨çš„ tobemodified æª”æ¡ˆ
    available_files = detect_tobemodified_files(config)
    
    if not available_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• tobemodified æª”æ¡ˆ")
        print("è«‹å…ˆåŸ·è¡Œ script_01_generate_xlsx.py ç”Ÿæˆæª”æ¡ˆ")
        sys.exit(1)
    
    # æ­¥é©Ÿ1ï¼šé¸æ“‡ tobemodified æª”æ¡ˆï¼ˆæ”¯æ´å¤šé¸ï¼‰
    selected_files = choose_tobemodified_files(available_files)
    if not selected_files:
        sys.exit(1)
    
    # æª¢æŸ¥ i18n_combine ç›®éŒ„
    combine_dir = Path("i18n_combine")
    
    if not combine_dir.exists():
        print(f"âŒ åˆä½µç›®éŒ„ä¸å­˜åœ¨ï¼š{combine_dir}")
        print(f"è«‹å‰µå»ºç›®éŒ„ä¸¦æ”¾å…¥è¦åˆä½µçš„æª”æ¡ˆ")
        sys.exit(1)
    
    print(f"ğŸ“ æƒæåˆä½µç›®éŒ„ï¼š{combine_dir}")
    
    # æƒæ combine ç›®éŒ„ä¸­çš„æª”æ¡ˆ
    combine_files = scan_combine_directory(combine_dir)
    
    # æ­¥é©Ÿ2ï¼šé¸æ“‡è¦åˆä½µçš„ JSON æª”æ¡ˆ
    target_json_path = choose_combine_file(combine_files['json'], 'json')
    
    # æ­¥é©Ÿ3ï¼šé¸æ“‡è¦åˆä½µçš„ PO æª”æ¡ˆ
    target_po_path = choose_combine_file(combine_files['po'], 'po')
    
    # è®€å–æ‰€æœ‰é¸ä¸­èªè¨€çš„ Excel æ›´æ–°è³‡æ–™
    all_updates = {}
    detected_languages = []
    
    for language, xlsx_path in selected_files.items():
        updates = read_excel_updates_for_language(xlsx_path, language, config)
        if updates:
            all_updates[language] = updates
            detected_languages.append(language)
    
    if not all_updates:
        print("âŒ æ²’æœ‰è®€å–åˆ°ä»»ä½•æœ‰æ•ˆçš„æ›´æ–°è³‡æ–™")
        sys.exit(1)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æ›´æ–°
    has_json_updates = check_json_updates_exist(all_updates)
    has_po_updates = check_po_updates_exist(all_updates)
    
    # å¦‚æœæ²’æœ‰é¸æ“‡ JSON æª”æ¡ˆä½†æœ‰ JSON æ›´æ–°ï¼Œè©¢å•æ˜¯å¦å‰µå»ºæ–°æª”æ¡ˆ
    if not target_json_path and has_json_updates:
        print(f"\nğŸ’¡ æª¢æ¸¬åˆ° JSON æ›´æ–°ä½†æœªé¸æ“‡ç›®æ¨™æª”æ¡ˆ")
        while True:
            try:
                choice = input(f"æ˜¯å¦å‰µå»ºæ–°çš„å¤šèªè¨€ JSON æª”æ¡ˆï¼Ÿ(Y/n)ï¼š").strip().lower()
                if choice in ['', 'y', 'yes']:
                    target_json_path = "CREATE_NEW"
                    print(f"âœ… å°‡å‰µå»ºæ–°çš„å¤šèªè¨€ JSON æª”æ¡ˆ")
                    break
                elif choice in ['n', 'no']:
                    print(f"â­ï¸  è·³é JSON æª”æ¡ˆè™•ç†")
                    break
                else:
                    print(f"âš ï¸  è«‹è¼¸å…¥ Y æˆ– N")
            except KeyboardInterrupt:
                print(f"\nâŒ æ“ä½œå–æ¶ˆ")
                target_json_path = None
                break
    
    # å¦‚æœæ²’æœ‰é¸æ“‡ PO æª”æ¡ˆä½†æœ‰ PO æ›´æ–°ï¼Œè©¢å•æ˜¯å¦å‰µå»ºæ–°æª”æ¡ˆ
    if not target_po_path and has_po_updates:
        print(f"\nğŸ’¡ æª¢æ¸¬åˆ° PO æ›´æ–°ä½†æœªé¸æ“‡ç›®æ¨™æª”æ¡ˆ")
        while True:
            try:
                choice = input(f"æ˜¯å¦å‰µå»ºæ–°çš„ messages.po æª”æ¡ˆï¼Ÿ(Y/n)ï¼š").strip().lower()
                if choice in ['', 'y', 'yes']:
                    target_po_path = "CREATE_NEW"
                    print(f"âœ… å°‡å‰µå»ºæ–°çš„ messages.po æª”æ¡ˆ")
                    break
                elif choice in ['n', 'no']:
                    print(f"â­ï¸  è·³é PO æª”æ¡ˆè™•ç†")
                    break
                else:
                    print(f"âš ï¸  è«‹è¼¸å…¥ Y æˆ– N")
            except KeyboardInterrupt:
                print(f"\nâŒ æ“ä½œå–æ¶ˆ")
                target_po_path = None
                break
    
    # æª¢æŸ¥æ˜¯å¦è‡³å°‘é¸æ“‡äº†ä¸€å€‹æª”æ¡ˆæˆ–æœ‰æ›´æ–°éœ€è¦è™•ç†
    if not target_json_path and not target_po_path:
        print("âŒ å¿…é ˆè‡³å°‘é¸æ“‡ä¸€å€‹æª”æ¡ˆé€²è¡Œåˆä½µ")
        sys.exit(1)
    
    # çµ±è¨ˆæ‰€æœ‰æ¥­æ…‹
    all_business_types = set()
    for language_updates in all_updates.values():
        all_business_types.update(language_updates.keys())
    
    print(f"\nğŸ“‹ åˆä½µè¨­å®šï¼š")
    print(f"   ä¾†æºèªè¨€ï¼š{', '.join(selected_files.keys())}")
    if target_json_path:
        if target_json_path == "CREATE_NEW":
            print(f"   JSON æª”æ¡ˆï¼šå°‡å‰µå»ºæ–°çš„å¤šèªè¨€ JSON")
        else:
            print(f"   JSON æª”æ¡ˆï¼š{target_json_path.relative_to(combine_dir)}")
    if target_po_path:
        if target_po_path == "CREATE_NEW":
            print(f"   PO æª”æ¡ˆï¼šå°‡å‰µå»ºæ–°çš„ messages.po")
        else:
            print(f"   PO æª”æ¡ˆï¼š{target_po_path.relative_to(combine_dir)}")
    print(f"   æ¶µè“‹æ¥­æ…‹ï¼š{', '.join([config.get_business_types()[bt]['display_name'] for bt in all_business_types])}")
    
    # é¡¯ç¤ºé™£åˆ—æ›´æ–°åŠŸèƒ½æç¤º
    print(f"\nğŸ”§ v1.6 æ–°åŠŸèƒ½ï¼šæ™ºèƒ½é™£åˆ—è™•ç†")
    print(f"   - è‡ªå‹•æª¢æ¸¬é™£åˆ—ç´¢å¼•è·¯å¾‘ï¼ˆå¦‚ slogan[1]ï¼‰")
    print(f"   - å¾ i18n_input/{{language}}/{{language}}.json è®€å–åŸå§‹é™£åˆ—")
    print(f"   - é€²è¡Œå®Œæ•´é™£åˆ—æ›´æ–°ï¼Œé¿å…å…¶ä»–ä½ç½®è®Šæˆ null")
    
    # æª¢æŸ¥ i18n_input ç›®éŒ„
    input_dir = Path("i18n_input")
    if not input_dir.exists():
        print(f"âš ï¸  æœªæ‰¾åˆ° i18n_input ç›®éŒ„ï¼Œé™£åˆ—æ›´æ–°åŠŸèƒ½å¯èƒ½å—é™")
    else:
        missing_languages = []
        for language in detected_languages:
            language_file = input_dir / language / f"{language}.json"
            if not language_file.exists():
                missing_languages.append(language)
        
        if missing_languages:
            print(f"âš ï¸  ç¼ºå°‘åŸå§‹èªè¨€æª”æ¡ˆï¼š{', '.join(missing_languages)}")
            print(f"   é™£åˆ—æ›´æ–°å°‡é™ç´šç‚ºå‚³çµ±ç´¢å¼•æ›´æ–°")
        else:
            print(f"âœ… æ‰€æœ‰èªè¨€çš„åŸå§‹æª”æ¡ˆéƒ½å·²æ‰¾åˆ°ï¼Œæ”¯æ´å®Œæ•´é™£åˆ—æ›´æ–°")
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    dirs = config.get_directories()
    output_dir = Path(dirs['output_dir']) / f"multi_{timestamp}_combined"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¨­ç½®æ—¥èªŒ
    log_file = output_dir / f"multi_combine_{timestamp}.log"
    
    def log_detail(message: str):
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"{datetime.datetime.now().strftime('%H:%M:%S')} - {message}\n")
    
    log_detail(f"é–‹å§‹å¤šèªè¨€åˆä½µè™•ç† (v1.6)")
    log_detail(f"èªè¨€ï¼š{', '.join(selected_files.keys())}")
    log_detail(f"ä¾†æºæª”æ¡ˆï¼š{list(selected_files.values())}")
    log_detail(f"æ¶µè“‹æ¥­æ…‹ï¼š{', '.join(all_business_types)}")
    log_detail(f"é™£åˆ—æ›´æ–°åŠŸèƒ½ï¼šå•Ÿç”¨")
    
    # è™•ç†åˆä½µé‚è¼¯ - é¿å…æ¥­æ…‹é–“è¡çª
    business_types = config.get_business_types()
    all_results = {}
    
    # æŒ‰æ¥­æ…‹åˆ†åˆ¥è™•ç†ï¼Œé¿å…ç›¸äº’å¹²æ“¾
    for bt_code in all_business_types:
        if bt_code not in business_types:
            continue
            
        bt_config = business_types[bt_code]
        display_name = bt_config['display_name']
        suffix = bt_config['suffix']
        
        print(f"\nğŸ“ è™•ç† {display_name}...")
        log_detail(f"é–‹å§‹è™•ç†æ¥­æ…‹ï¼š{display_name}")
        
        results = {}
        
        # ç‚ºç•¶å‰æ¥­æ…‹è™•ç† JSON æª”æ¡ˆ
        if target_json_path:
            if target_json_path == "CREATE_NEW":
                output_json_path = output_dir / f"multilang{suffix}_combined.json"
            else:
                output_json_path = output_dir / f"{target_json_path.stem}{suffix}_combined.json"
            
            create_new = (target_json_path == "CREATE_NEW")
            json_result = combine_multilang_json_files_for_business_type(
                all_updates,
                target_json_path if not create_new else None,
                output_json_path,
                bt_code,
                log_detail,
                create_new,
                detected_languages
            )
            results['json_result'] = json_result
            
            # é¡¯ç¤ºçµæœ
            if json_result.get('errors'):
                print(f"     âŒ JSON æª”æ¡ˆè™•ç†éŒ¯èª¤ï¼š{json_result['errors']}")
            else:
                # é¡¯ç¤ºèªè¨€çµ±è¨ˆ
                if json_result.get('language_stats'):
                    for lang, stats in json_result['language_stats'].items():
                        if stats['merged'] > 0 or stats['skipped'] > 0:
                            print(f"     ğŸ“Š {lang}: åˆä½µ {stats['merged']} å€‹, è·³é {stats['skipped']} å€‹")
                
                if json_result.get('created_new'):
                    print(f"     ğŸ†• å‰µå»ºäº†æ–°çš„ JSON æª”æ¡ˆ")
                
                if json_result.get('merged', 0) == 0 and json_result.get('skipped', 0) == 0:
                    if not json_result.get('created_new'):
                        print(f"     â„¹ï¸  {display_name} æ²’æœ‰ JSON æ›´æ–°é …ç›®")
        
        # ç‚ºç•¶å‰æ¥­æ…‹è™•ç† PO æª”æ¡ˆ
        if target_po_path:
            create_new = (target_po_path == "CREATE_NEW")
            po_result = combine_po_files_for_business_type(
                all_updates,
                target_po_path if not create_new else None,
                output_dir,
                bt_code,
                log_detail,
                create_new
            )
            results['po_result'] = po_result
            
            # é¡¯ç¤ºçµæœ
            if po_result.get('errors'):
                print(f"     âŒ PO æª”æ¡ˆè™•ç†éŒ¯èª¤ï¼š{po_result['errors']}")
            else:
                # é¡¯ç¤ºèªè¨€çµ±è¨ˆ
                if po_result.get('language_stats'):
                    for lang, stats in po_result['language_stats'].items():
                        if stats['merged'] > 0 or stats['skipped'] > 0:
                            print(f"     ğŸ“Š {lang}: åˆä½µ {stats['merged']} å€‹, è·³é {stats['skipped']} å€‹")
                
                if po_result.get('created_new'):
                    print(f"     ğŸ†• å‰µå»ºäº†æ–°çš„ PO æª”æ¡ˆ")
                
                if po_result.get('merged', 0) == 0 and po_result.get('skipped', 0) == 0:
                    if not po_result.get('created_new'):
                        print(f"     â„¹ï¸  {display_name} æ²’æœ‰ PO æ›´æ–°é …ç›®")
        
        # å¦‚æœæ²’æœ‰æ›´æ–°ï¼Œè¤‡è£½åŸæª”æ¡ˆï¼ˆåƒ…é™éå‰µå»ºæ–°æª”æ¡ˆçš„æƒ…æ³ï¼‰
        if target_json_path and target_json_path != "CREATE_NEW" and results.get('json_result', {}).get('merged', 0) == 0:
            if not results.get('json_result', {}).get('created_new', False):
                output_json_path = output_dir / f"{target_json_path.stem}{suffix}_combined.json"
                if not output_json_path.exists():
                    output_json_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(target_json_path, output_json_path)
                    print(f"     ğŸ“„ è¤‡è£½ JSON æª”æ¡ˆï¼ˆç„¡æ›´æ–°ï¼‰")
                    log_detail(f"è¤‡è£½åŸå§‹ JSON æª”æ¡ˆï¼š{target_json_path.name}")
        
        # PO æª”æ¡ˆç¾åœ¨æ˜¯æŒ‰èªè¨€åˆ†åˆ¥ç”Ÿæˆï¼Œæ‰€ä»¥ä¸éœ€è¦è¤‡è£½é‚è¼¯
        
        all_results[bt_code] = results
        
        # çµ±è¨ˆçµæœ
        total_merged = 0
        total_skipped = 0
        total_errors = 0
        has_new_files = False
        
        for result in results.values():
            total_merged += result.get('merged', 0)
            total_skipped += result.get('skipped', 0)
            total_errors += len(result.get('errors', []))
            if result.get('created_new'):
                has_new_files = True
        
        if total_errors > 0:
            print(f"     âŒ è™•ç†å¤±æ•— - éŒ¯èª¤: {total_errors} å€‹")
        else:
            status_msg = f"å®Œæˆ - åˆä½µ: {total_merged} å€‹, è·³é: {total_skipped} å€‹"
            if has_new_files:
                status_msg += " (åŒ…å«æ–°æª”æ¡ˆ)"
            print(f"     âœ… {status_msg}")
        
        log_detail(f"{display_name} è™•ç†å®Œæˆï¼šåˆä½µ {total_merged} å€‹ï¼Œè·³é {total_skipped} å€‹ï¼ŒéŒ¯èª¤ {total_errors} å€‹")
    
    # ç”Ÿæˆæœ€çµ‚å ±å‘Š
    total_merged = sum(
        sum(result.get('merged', 0) for result in results.values())
        for results in all_results.values()
    )
    total_skipped = sum(
        sum(result.get('skipped', 0) for result in results.values())
        for results in all_results.values()
    )
    total_errors = sum(
        sum(len(result.get('errors', [])) for result in results.values())
        for results in all_results.values()
    )
    
    print(f"\nğŸ‰ å¤šèªè¨€åˆä½µè™•ç†å®Œæˆï¼(v1.6)")
    print(f"ğŸ“Š è™•ç†çµæœï¼šåˆä½µ {total_merged} å€‹é …ç›®ï¼Œè·³é {total_skipped} å€‹é …ç›®")
    if total_errors > 0:
        print(f"âš ï¸  è™•ç†éŒ¯èª¤ï¼š{total_errors} å€‹")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„ï¼š{output_dir}")
    print(f"ğŸ”§ é™£åˆ—æ›´æ–°åŠŸèƒ½ï¼šå·²å•Ÿç”¨ï¼Œè‡ªå‹•è™•ç†é™£åˆ—ç´¢å¼•è·¯å¾‘")
    
    # ç”Ÿæˆè™•ç†æ‘˜è¦
    generate_multilang_summary_report(all_results, all_updates, output_dir, timestamp, target_json_path, target_po_path, log_detail)


if __name__ == "__main__":
    main()