#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
script_02_apply_fixes_partial.py (v1.0 - éƒ¨åˆ†æª”æ¡ˆç”Ÿæˆç‰ˆ)

ä¿®æ”¹å…§å®¹ï¼š
1. âœ… åªç”Ÿæˆæœ‰ä¿®æ”¹çš„é …ç›®åˆ°éƒ¨åˆ†æª”æ¡ˆï¼ˆmessages_partial.po å’Œ {language}_partial.jsonï¼‰
2. âœ… ä¿æŒåŸæœ‰æª”æ¡ˆçµæ§‹å’Œå±¤ç´šï¼Œç¢ºä¿ i18n ç›¸å®¹æ€§
3. âœ… æ”¯æ´å¤šé‡æ•æ„Ÿè©å’Œæ–°æ ¼å¼
4. âœ… è‡ªå‹•è·³éç©ºçš„æ›¿æ›çµæœï¼Œé¿å…ç„¡æ„ç¾©çš„è™•ç†
5. âœ… ç”Ÿæˆè©³ç´°çš„è™•ç†å ±å‘Š

ä¾æ“šå„èªè¨€çš„ tobemodified_{language}.xlsxï¼Œåƒ…å°‡æœ‰ä¿®æ­£çš„é …ç›®å¯«å…¥éƒ¨åˆ†æª”æ¡ˆ
"""

import json
import sys
import datetime
import argparse
import glob
from pathlib import Path
from collections import defaultdict
from config_loader import get_config

try:
    import openpyxl
    import polib
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦å¥—ä»¶ï¼š{e}")
    print("è«‹åŸ·è¡Œï¼špip install openpyxl polib")
    sys.exit(1)


def read_and_validate_xlsx(xlsx_path: Path, config, target_business_types: list, log_detail) -> tuple:
    """è®€å–ä¸¦é©—è­‰ Excel æª”æ¡ˆ"""
    try:
        log_detail(f"é–‹å§‹è®€å– Excel æª”æ¡ˆ: {xlsx_path}")
        wb = openpyxl.load_workbook(xlsx_path, data_only=True)
        ws = wb.active
        
        header_row = list(ws[1])
        header = {cell.value: idx for idx, cell in enumerate(header_row) if cell.value}
        
        log_detail(f"ç™¼ç¾æ¬„ä½: {list(header.keys())}")
        
        # åŸºæœ¬æ¬„ä½æª¢æŸ¥
        required_columns = ["æª”æ¡ˆé¡å‹", "é …ç›®ID", "é …ç›®å…§å®¹", "æ•æ„Ÿè©"]
        missing_columns = []
        
        for col in required_columns:
            if col not in header:
                missing_columns.append(col)
        
        # æª¢æŸ¥æ¥­æ…‹æ›¿æ›çµæœæ¬„ä½
        business_types = config.get_business_types()
        business_result_columns = []
        
        for bt_code in target_business_types:
            display_name = business_types[bt_code]['display_name']
            result_col_name = f"{display_name}_æ›¿æ›çµæœ"
            if result_col_name not in header:
                missing_columns.append(result_col_name)
            else:
                business_result_columns.append(result_col_name)
        
        if missing_columns:
            error_msg = f"Excel ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}"
            print(f"âŒ {error_msg}")
            log_detail(f"éŒ¯èª¤: {error_msg}")
            return None, None, None
        
        log_detail(f"æ¥­æ…‹æ›¿æ›çµæœæ¬„ä½: {business_result_columns}")
        
        return wb, ws, header
        
    except Exception as e:
        error_msg = f"è®€å– Excel æª”æ¡ˆå¤±æ•—ï¼š{e}"
        print(f"âŒ {error_msg}")
        log_detail(f"éŒ¯èª¤: {error_msg}")
        return None, None, None


def parse_excel_updates(ws, header, config, target_business_types: list, log_detail) -> dict:
    """è§£æ Excel ä¸­çš„ä¿®æ­£è³‡æ–™ï¼Œåªè¿”å›æœ‰æœ‰æ•ˆæ›¿æ›çš„é …ç›®"""
    log_detail("é–‹å§‹è§£æ Excel ä¿®æ­£è³‡æ–™")
    updates = {bt_code: {"po": [], "json": []} for bt_code in target_business_types}
    stats = defaultdict(int)
    
    def get_column_index(name: str) -> int:
        if name not in header:
            raise KeyError(f"Excel ç¼ºå°‘æ¬„ä½ï¼š{name}")
        return header[name]
    
    def get_optional_column_index(name: str) -> int:
        """ç²å–å¯é¸æ¬„ä½ç´¢å¼•ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› -1"""
        return header.get(name, -1)
    
    business_types = config.get_business_types()
    
    # ç²å–å¯é¸æ¬„ä½ç´¢å¼•
    match_pos_idx = get_optional_column_index("åŒ¹é…ä½ç½®")
    category_idx = get_optional_column_index("æ•æ„Ÿè©åˆ†é¡")
    
    # çµ±è¨ˆè®Šæ•¸
    skipped_empty_replacements = 0
    skipped_same_as_original = 0
    valid_updates = 0
    
    for row_num, row in enumerate(ws.iter_rows(min_row=2, values_only=True), start=2):
        if not row or len(row) <= max(header.values()):
            continue
        
        try:
            file_type = row[get_column_index("æª”æ¡ˆé¡å‹")]
            entry_id = row[get_column_index("é …ç›®ID")]
            original_text = row[get_column_index("é …ç›®å…§å®¹")]
            sensitive_word = row[get_column_index("æ•æ„Ÿè©")]
            
            if not file_type or not entry_id:
                continue
            
            file_type = str(file_type).lower()
            stats['total_rows'] += 1
            
            # è®€å–èª¿è©¦ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            debug_info = {}
            if match_pos_idx >= 0 and match_pos_idx < len(row) and row[match_pos_idx]:
                debug_info['match_position'] = str(row[match_pos_idx])
            
            if category_idx >= 0 and category_idx < len(row) and row[category_idx]:
                debug_info['category'] = str(row[category_idx])
            
            # è§£æå¤šé‡æ•æ„Ÿè©ï¼ˆä»¥é€—è™Ÿåˆ†éš”ï¼‰
            if sensitive_word:
                sensitive_words_list = [w.strip() for w in str(sensitive_word).split(',') if w.strip()]
                debug_info['multiple_sensitive_words'] = sensitive_words_list
                log_detail(f"è¡Œ {row_num}: æª¢æ¸¬åˆ° {len(sensitive_words_list)} å€‹æ•æ„Ÿè©: {sensitive_words_list}")
            
            # è™•ç†æ¯å€‹ç›®æ¨™æ¥­æ…‹
            has_valid_update = False
            for bt_code in target_business_types:
                display_name = business_types[bt_code]['display_name']
                result_col_name = f"{display_name}_æ›¿æ›çµæœ"
                
                try:
                    new_value = row[get_column_index(result_col_name)]
                except KeyError:
                    continue
                
                # åš´æ ¼çš„ç©ºå€¼æª¢æŸ¥ï¼Œè·³éç©ºç™½å€¼
                if not new_value or not str(new_value).strip():
                    skipped_empty_replacements += 1
                    log_detail(f"è¡Œ {row_num}: è·³éç©ºçš„æ›¿æ›çµæœ ({display_name})")
                    continue
                
                new_value = str(new_value).strip()
                
                # å®‰å…¨æª¢æŸ¥ï¼šè·³éèˆ‡åŸæ–‡ç›¸åŒçš„æ›¿æ›çµæœ
                if original_text and str(original_text).strip() == new_value:
                    skipped_same_as_original += 1
                    log_detail(f"è¡Œ {row_num}: è·³éèˆ‡åŸæ–‡ç›¸åŒçš„æ›¿æ›çµæœ ({display_name})")
                    continue
                
                # è¨˜éŒ„æœ‰æ•ˆçš„æ›´æ–°
                stats[f'{bt_code}_updates'] += 1
                has_valid_update = True
                
                # å‰µå»ºæ›´æ–°è¨˜éŒ„
                update_record = (str(entry_id), new_value, debug_info)
                
                if file_type == "po":
                    updates[bt_code]["po"].append(update_record)
                elif file_type == "json":
                    updates[bt_code]["json"].append(update_record)
            
            if has_valid_update:
                valid_updates += 1
        
        except Exception as e:
            log_detail(f"éŒ¯èª¤: ç¬¬ {row_num} è¡Œè™•ç†å¤±æ•—: {e}")
            continue
    
    # çµ±è¨ˆå ±å‘Š
    total_updates = sum(stats[f'{bt_code}_updates'] for bt_code in target_business_types if f'{bt_code}_updates' in stats)
    log_detail(f"è§£æå®Œæˆ - æœ‰æ•ˆæ›´æ–°é …ç›®æ•¸: {valid_updates}")
    log_detail(f"ç¸½æ›´æ–°æ“ä½œæ•¸: {total_updates}")
    log_detail(f"è·³éçµ±è¨ˆ - ç©ºæ›¿æ›çµæœ: {skipped_empty_replacements}, èˆ‡åŸæ–‡ç›¸åŒ: {skipped_same_as_original}")
    
    # åœ¨æ§åˆ¶å°é¡¯ç¤ºé—œéµçµ±è¨ˆ
    print(f"   ğŸ“Š æœ‰æ•ˆé …ç›®ï¼š{valid_updates} å€‹ï¼Œç¸½æ“ä½œï¼š{total_updates} æ¬¡")
    if skipped_empty_replacements > 0 or skipped_same_as_original > 0:
        print(f"   ğŸ“Š å®‰å…¨è·³éï¼šç©ºæ›¿æ› {skipped_empty_replacements} å€‹ï¼Œç„¡è®ŠåŒ– {skipped_same_as_original} å€‹")
    
    return updates


def create_partial_po_file(original_po_path: Path, updates_list: list, output_path: Path, log_detail) -> dict:
    """å‰µå»ºéƒ¨åˆ† PO æª”æ¡ˆï¼ŒåªåŒ…å«æœ‰ä¿®æ”¹çš„é …ç›®"""
    result = {"success": False, "updated": 0, "errors": [], "details": []}
    
    if not updates_list:
        log_detail("æ²’æœ‰ PO æ›´æ–°é …ç›®ï¼Œè·³ééƒ¨åˆ†æª”æ¡ˆç”Ÿæˆ")
        return {"success": True, "updated": 0, "errors": [], "details": []}
    
    try:
        # è¼‰å…¥åŸå§‹ PO æª”æ¡ˆ
        original_po = polib.pofile(str(original_po_path))
        
        # å‰µå»ºæ–°çš„ PO æª”æ¡ˆï¼Œä¿æŒåŸæœ‰çš„å…ƒä¿¡æ¯
        partial_po = polib.POFile()
        
        # è¤‡è£½å…ƒä¿¡æ¯
        partial_po.metadata = original_po.metadata.copy()
        partial_po.header = original_po.header
        
        # æ·»åŠ éƒ¨åˆ†æª”æ¡ˆæ¨™è­˜åˆ°æ¨™é¡Œ
        if 'POT-Creation-Date' in partial_po.metadata:
            creation_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M%z')
            partial_po.metadata['POT-Creation-Date'] = creation_date
        
        # æ·»åŠ è‡ªå®šç¾©æ¨™é¡Œè¨»è§£
        if partial_po.header:
            partial_po.header += f"\n# é€™æ˜¯éƒ¨åˆ†æª”æ¡ˆï¼ŒåªåŒ…å«æœ‰ä¿®æ”¹çš„é …ç›®\n# ç”Ÿæˆæ™‚é–“ï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        
        # è™•ç†æ›´æ–°é …ç›®
        for update_record in updates_list:
            # å…¼å®¹èˆŠæ ¼å¼å’Œæ–°æ ¼å¼
            if len(update_record) == 2:
                msgid, new_msgstr = update_record
                debug_info = {}
            elif len(update_record) == 3:
                msgid, new_msgstr, debug_info = update_record
            else:
                continue
            
            # åœ¨åŸå§‹æª”æ¡ˆä¸­æŸ¥æ‰¾å°æ‡‰çš„é …ç›®
            original_entry = original_po.find(msgid)
            if original_entry:
                # é¡å¤–çš„å®‰å…¨æª¢æŸ¥
                if original_entry.msgstr == new_msgstr:
                    log_detail(f"PO è·³é: '{msgid}' å…§å®¹ç„¡è®ŠåŒ–")
                    continue
                
                # å‰µå»ºæ–°çš„æ¢ç›®ï¼Œä¿æŒåŸæœ‰çš„å…ƒä¿¡æ¯
                new_entry = polib.POEntry(
                    msgid=original_entry.msgid,
                    msgstr=new_msgstr,
                    msgctxt=original_entry.msgctxt,
                    msgid_plural=original_entry.msgid_plural,
                    msgstr_plural=original_entry.msgstr_plural,
                    obsolete=original_entry.obsolete,
                    encoding=original_entry.encoding,
                    comment=original_entry.comment,
                    tcomment=original_entry.tcomment,
                    occurrences=original_entry.occurrences,
                    flags=original_entry.flags
                )
                
                # æ·»åŠ è™•ç†ä¿¡æ¯åˆ°è¨»è§£
                if debug_info.get('multiple_sensitive_words'):
                    sensitive_words = ', '.join(debug_info['multiple_sensitive_words'])
                    new_entry.tcomment += f"\n# æ•æ„Ÿè©: {sensitive_words}"
                
                if debug_info.get('category'):
                    new_entry.tcomment += f"\n# åˆ†é¡: {debug_info['category']}"
                
                partial_po.append(new_entry)
                result["updated"] += 1
                
                # è¨˜éŒ„è©³ç´°ä¿¡æ¯
                detail_msg = f"PO æ›´æ–°: '{msgid}'"
                if debug_info.get('multiple_sensitive_words'):
                    sensitive_count = len(debug_info['multiple_sensitive_words'])
                    detail_msg += f" [æ•æ„Ÿè©:{sensitive_count}å€‹]"
                
                detail_msg += f" â†’ '{new_msgstr[:50]}{'...' if len(new_msgstr) > 50 else ''}'"
                result["details"].append(detail_msg)
                log_detail(detail_msg)
            else:
                error_msg = f"æ‰¾ä¸åˆ°æ¢ç›®ï¼š{msgid}"
                result["errors"].append(error_msg)
                log_detail(f"PO éŒ¯èª¤: {error_msg}")
        
        # ä¿å­˜éƒ¨åˆ†æª”æ¡ˆ
        if result["updated"] > 0:
            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            output_path.parent.mkdir(parents=True, exist_ok=True)
            partial_po.save(str(output_path))
            log_detail(f"éƒ¨åˆ† PO æª”æ¡ˆå·²å„²å­˜: {output_path.name}, åŒ…å« {result['updated']} å€‹æ¢ç›®")
        
        result["success"] = True
        
    except Exception as e:
        error_msg = f"PO éƒ¨åˆ†æª”æ¡ˆç”Ÿæˆå¤±æ•—ï¼š{e}"
        result["errors"].append(error_msg)
        log_detail(f"PO éŒ¯èª¤: {error_msg}")
    
    return result


def create_partial_json_file(original_json_path: Path, updates_list: list, output_path: Path, log_detail) -> dict:
    """å‰µå»ºéƒ¨åˆ† JSON æª”æ¡ˆï¼ŒåªåŒ…å«æœ‰ä¿®æ”¹çš„é …ç›®"""
    result = {"success": False, "updated": 0, "errors": [], "details": []}
    
    if not updates_list:
        log_detail("æ²’æœ‰ JSON æ›´æ–°é …ç›®ï¼Œè·³ééƒ¨åˆ†æª”æ¡ˆç”Ÿæˆ")
        return {"success": True, "updated": 0, "errors": [], "details": []}
    
    try:
        # è¼‰å…¥åŸå§‹ JSON æª”æ¡ˆ
        original_data = json.loads(original_json_path.read_text(encoding="utf-8"))
        
        # å‰µå»ºéƒ¨åˆ†æ•¸æ“šçµæ§‹
        partial_data = {}
        
        # æ·»åŠ å…ƒä¿¡æ¯
        partial_data["_metadata"] = {
            "type": "partial_translation",
            "source_file": str(original_json_path),
            "generated_at": datetime.datetime.now().isoformat(),
            "description": "This file contains only modified translation entries"
        }
        
        # è™•ç†æ›´æ–°é …ç›®
        for update_record in updates_list:
            # å…¼å®¹èˆŠæ ¼å¼å’Œæ–°æ ¼å¼
            if len(update_record) == 2:
                json_path_str, new_value = update_record
                debug_info = {}
            elif len(update_record) == 3:
                json_path_str, new_value, debug_info = update_record
            else:
                continue
            
            # ç²å–åŸå§‹å€¼é€²è¡Œæ¯”è¼ƒ
            original_value = get_json_value_by_path(original_data, json_path_str)
            
            # é¡å¤–çš„å®‰å…¨æª¢æŸ¥
            if original_value == new_value:
                log_detail(f"JSON è·³é: '{json_path_str}' å…§å®¹ç„¡è®ŠåŒ–")
                continue
            
            # è¨­ç½®æ–°å€¼åˆ°éƒ¨åˆ†æ•¸æ“šçµæ§‹ä¸­
            if set_json_value_by_path(partial_data, json_path_str, new_value):
                result["updated"] += 1
                
                # è¨˜éŒ„è©³ç´°ä¿¡æ¯
                detail_msg = f"JSON æ›´æ–°: '{json_path_str}'"
                if debug_info.get('multiple_sensitive_words'):
                    sensitive_count = len(debug_info['multiple_sensitive_words'])
                    detail_msg += f" [æ•æ„Ÿè©:{sensitive_count}å€‹]"
                
                detail_msg += f" â†’ '{new_value[:50]}{'...' if len(new_value) > 50 else ''}'"
                result["details"].append(detail_msg)
                log_detail(detail_msg)
            else:
                error_msg = f"ç„¡æ³•è¨­ç½®è·¯å¾‘ï¼š{json_path_str}"
                result["errors"].append(error_msg)
                log_detail(f"JSON éŒ¯èª¤: {error_msg}")
        
        # ä¿å­˜éƒ¨åˆ†æª”æ¡ˆ
        if result["updated"] > 0:
            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            json_content = json.dumps(partial_data, ensure_ascii=False, indent=2)
            output_path.write_text(json_content, encoding="utf-8")
            log_detail(f"éƒ¨åˆ† JSON æª”æ¡ˆå·²å„²å­˜: {output_path.name}, åŒ…å« {result['updated']} å€‹æ¢ç›®")
        
        result["success"] = True
        
    except json.JSONDecodeError as e:
        error_msg = f"JSON æ ¼å¼éŒ¯èª¤ï¼š{e}"
        result["errors"].append(error_msg)
        log_detail(f"JSON éŒ¯èª¤: {error_msg}")
    except Exception as e:
        error_msg = f"JSON éƒ¨åˆ†æª”æ¡ˆç”Ÿæˆå¤±æ•—ï¼š{e}"
        result["errors"].append(error_msg)
        log_detail(f"JSON éŒ¯èª¤: {error_msg}")
    
    return result


def get_json_value_by_path(data: dict, path: str):
    """æŒ‰è·¯å¾‘ç²å– JSON å€¼"""
    try:
        path_parts = parse_json_path(path)
        current = data
        
        for part_type, part_value in path_parts:
            if part_type == 'key':
                if part_value not in current:
                    return None
                current = current[part_value]
            elif part_type == 'index':
                if not isinstance(current, list) or len(current) <= part_value:
                    return None
                current = current[part_value]
        
        return current
        
    except Exception:
        return None


def parse_json_path(path: str) -> list:
    """è§£æ JSON è·¯å¾‘"""
    parts = []
    current = ""
    in_bracket = False
    
    for char in path:
        if char == '[':
            if current:
                parts.append(('key', current))
                current = ""
            in_bracket = True
        elif char == ']':
            if in_bracket and current:
                try:
                    parts.append(('index', int(current)))
                except ValueError:
                    raise ValueError(f"ç„¡æ•ˆçš„é™£åˆ—ç´¢å¼•ï¼š{current}")
                current = ""
            in_bracket = False
        elif char == '.' and not in_bracket:
            if current:
                parts.append(('key', current))
                current = ""
        else:
            current += char
    
    if current:
        parts.append(('key', current))
    
    return parts


def set_json_value_by_path(data: dict, path: str, new_value: str) -> bool:
    """æŒ‰è·¯å¾‘è¨­ç½® JSON å€¼"""
    try:
        path_parts = parse_json_path(path)
        current = data
        
        for i, (part_type, part_value) in enumerate(path_parts):
            is_last = (i == len(path_parts) - 1)
            
            if part_type == 'key':
                if is_last:
                    current[part_value] = new_value
                else:
                    if part_value not in current:
                        next_part_type = path_parts[i + 1][0] if i + 1 < len(path_parts) else 'key'
                        current[part_value] = [] if next_part_type == 'index' else {}
                    current = current[part_value]
            
            elif part_type == 'index':
                if is_last:
                    while len(current) <= part_value:
                        current.append(None)
                    current[part_value] = new_value
                else:
                    while len(current) <= part_value:
                        current.append(None)
                    if current[part_value] is None:
                        next_part_type = path_parts[i + 1][0] if i + 1 < len(path_parts) else 'key'
                        current[part_value] = [] if next_part_type == 'index' else {}
                    current = current[part_value]
        
        return True
        
    except Exception as e:
        return False


def detect_tobemodified_files(config) -> dict:
    """æª¢æ¸¬å¯ç”¨çš„ tobemodified æª”æ¡ˆ"""
    available_files = {}
    
    # æª¢æ¸¬è¼¸å‡ºç›®éŒ„ä¸­çš„æª”æ¡ˆ
    try:
        if hasattr(config, 'get_output_dir'):
            output_dir = config.get_output_dir()
        elif hasattr(config, 'output_dir'):
            output_dir = config.output_dir
        elif hasattr(config, 'get_config'):
            config_data = config.get_config()
            output_dir = Path(config_data.get('output_dir', 'i18n_output'))
        else:
            output_dir = Path('i18n_output')
    except Exception:
        output_dir = Path('i18n_output')
    
    # ä½¿ç”¨é…ç½®è¼‰å…¥å™¨çš„èªè¨€æª¢æ¸¬
    try:
        available_languages = config.detect_available_languages()
    except Exception as e:
        print(f"âš ï¸  èªè¨€æª¢æ¸¬å¤±æ•—ï¼š{e}")
        available_languages = []
    
    # æª¢æ¸¬æ¨™æº–å‘½åçš„æª”æ¡ˆ
    for language in available_languages:
        tobemodified_path = output_dir / f"{language}_tobemodified.xlsx"
        if tobemodified_path.exists():
            available_files[language] = tobemodified_path
    
    # åœ¨ç•¶å‰ç›®éŒ„å’Œè¼¸å‡ºç›®éŒ„ä¸­æŸ¥æ‰¾é¡å¤–çš„ tobemodified æª”æ¡ˆ
    for search_dir in [output_dir]:
        if search_dir.exists():
            for file_path in search_dir.glob("*_tobemodified.xlsx"):
                filename = file_path.stem
                if filename.endswith('_tobemodified'):
                    language = filename[:-len('_tobemodified')]
                    
                    # éæ¿¾ç³»çµ±è‡¨æ™‚æª”æ¡ˆ
                    if language.startswith(('~$', '.', '__')):
                        continue
                    
                    if language not in available_files:
                        available_files[language] = file_path

    return available_files


def choose_business_types(config, args) -> list:
    """é¸æ“‡è¦è™•ç†çš„æ¥­æ…‹"""
    if args.business_types:
        if 'all' in args.business_types:
            return list(config.get_business_types().keys())
        return args.business_types
    
    # äº’å‹•å¼é¸æ“‡
    business_types = config.get_business_types()
    choices = list(business_types.items())
    
    print("\nè«‹é¸æ“‡è¦å¥—ç”¨ä¿®æ­£çš„æ¥­æ…‹ï¼š")
    for i, (bt_code, bt_config) in enumerate(choices, 1):
        print(f"  {i}) {bt_config['display_name']}")
    print(f"  {len(choices) + 1}) å…¨éƒ¨")
    
    while True:
        try:
            opt = input(f"\nè¼¸å…¥é¸é … (1-{len(choices) + 1})ï¼š").strip()
            choice_idx = int(opt) - 1
            
            if choice_idx == len(choices):  # å…¨éƒ¨
                return list(business_types.keys())
            elif 0 <= choice_idx < len(choices):
                bt_code = choices[choice_idx][0]
                return [bt_code]
            else:
                print(f"âš ï¸  è«‹è¼¸å…¥ 1-{len(choices) + 1} ä¹‹é–“çš„æ•¸å­—")
        except (ValueError, KeyboardInterrupt):
            print("\nâŒ ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ")
            sys.exit(0)


def process_language(config, language: str, target_business_types: list) -> bool:
    """è™•ç†å–®å€‹èªè¨€çš„éƒ¨åˆ†æª”æ¡ˆç”Ÿæˆ"""
    
    # ç²å–æª”æ¡ˆè·¯å¾‘
    available_files = detect_tobemodified_files(config)
    tobemodified_path = available_files.get(language)
    
    if not tobemodified_path:
        print(f"âŒ æ‰¾ä¸åˆ° {language} çš„ tobemodified æª”æ¡ˆ")
        return False
    
    language_files = config.get_language_files(language)
    
    print(f"   ä¾†æº Excelï¼š{tobemodified_path.name}")
    
    # ç²å–è¼¸å‡ºè·¯å¾‘
    try:
        output_paths = config.get_output_paths(language)
        output_dir = output_paths['output_dir']
        timestamp = output_paths['timestamp']
    except Exception:
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        try:
            if hasattr(config, 'get_output_dir'):
                base_output_dir = config.get_output_dir()
            else:
                base_output_dir = Path('i18n_output')
        except Exception:
            base_output_dir = Path('i18n_output')
        
        output_dir = base_output_dir / f"{language}_{timestamp}_partial"
    else:
        # ç‚ºéƒ¨åˆ†æª”æ¡ˆæ·»åŠ  _partial å¾Œç¶´
        output_dir = output_dir.parent / f"{output_dir.name}_partial"
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¨­ç½®æ—¥èªŒ
    log_file = output_dir / f"apply_fixes_partial_{timestamp}.log"
    
    def log_detail(message: str):
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"{datetime.datetime.now().strftime('%H:%M:%S')} - {message}\n")
    
    log_detail(f"é–‹å§‹è™•ç†èªè¨€: {language} (éƒ¨åˆ†æª”æ¡ˆæ¨¡å¼)")
    log_detail(f"ç›®æ¨™æ¥­æ…‹: {', '.join(target_business_types)}")
    log_detail(f"ä¾†æºæª”æ¡ˆ: {tobemodified_path}")
    
    # è®€å–ä¸¦é©—è­‰ Excel
    wb, ws, header = read_and_validate_xlsx(tobemodified_path, config, target_business_types, log_detail)
    if not wb:
        return False
    
    # è§£æä¿®æ­£è³‡æ–™
    updates = parse_excel_updates(ws, header, config, target_business_types, log_detail)
    
    # è™•ç†æ¯å€‹æ¥­æ…‹
    business_types = config.get_business_types()
    results = {}
    
    for bt_code in target_business_types:
        bt_config = business_types[bt_code]
        display_name = bt_config['display_name']
        suffix = bt_config['suffix']
        
        print(f"   ğŸ“ è™•ç† {display_name}...")
        log_detail(f"é–‹å§‹è™•ç†æ¥­æ…‹: {display_name}")
        
        # ç”Ÿæˆéƒ¨åˆ†æª”æ¡ˆ
        result = generate_partial_files(
            config, language, bt_code, updates[bt_code], language_files, output_dir, suffix, log_detail
        )
        
        results[bt_code] = result
        
        if result['success']:
            total_updates = result['po_updated'] + result['json_updated']
            print(f"     âœ… å®Œæˆ - PO: {result['po_updated']} å€‹, JSON: {result['json_updated']} å€‹")
            log_detail(f"{display_name} è™•ç†å®Œæˆ: ç¸½æ›´æ–° {total_updates} å€‹")
        else:
            print(f"     âŒ å¤±æ•—")
            log_detail(f"{display_name} è™•ç†å¤±æ•—")
            
            # è¨˜éŒ„éŒ¯èª¤è©³æƒ…åˆ°æ—¥èªŒ
            for error in result.get('errors', []):
                log_detail(f"  éŒ¯èª¤: {error}")
    
    # ç”Ÿæˆæœ€çµ‚å ±å‘Š
    success_count = sum(1 for r in results.values() if r['success'])
    total_count = len(results)
    total_updates = sum(r['po_updated'] + r['json_updated'] for r in results.values())
    
    print(f"   ğŸ“Š è™•ç†çµæœï¼šæˆåŠŸ {success_count}/{total_count}ï¼Œç¸½æ›´æ–° {total_updates} å€‹")
    print(f"   ğŸ“ è¼¸å‡ºç›®éŒ„ï¼š{output_dir}")
    
    log_detail(f"èªè¨€ {language} è™•ç†å®Œæˆ: æˆåŠŸ {success_count}/{total_count} å€‹æ¥­æ…‹")
    
    # ç”Ÿæˆè™•ç†æ‘˜è¦
    generate_summary_report(results, output_dir, timestamp, log_detail)
    
    return success_count > 0


def generate_partial_files(config, language: str, bt_code: str, updates: dict, language_files: dict, output_dir: Path, suffix: str, log_detail) -> dict:
    """ç”Ÿæˆéƒ¨åˆ†æª”æ¡ˆ"""
    result = {
        'success': True,
        'po_updated': 0,
        'json_updated': 0,
        'errors': [],
        'details': []
    }
    
    try:
        # ç”Ÿæˆéƒ¨åˆ† PO æª”æ¡ˆ
        if 'po_file' in language_files and updates['po']:
            original_po_path = language_files['po_file']
            partial_po_path = output_dir / f"messages{suffix}_partial.po"
            
            po_result = create_partial_po_file(original_po_path, updates['po'], partial_po_path, log_detail)
            result['po_updated'] = po_result['updated']
            result['errors'].extend(po_result['errors'])
            result['details'].extend(po_result.get('details', []))
            if not po_result['success']:
                result['success'] = False
        
        # ç”Ÿæˆéƒ¨åˆ† JSON æª”æ¡ˆ
        if 'json_file' in language_files and updates['json']:
            original_json_path = language_files['json_file']
            partial_json_path = output_dir / f"{language}{suffix}_partial.json"
            
            json_result = create_partial_json_file(original_json_path, updates['json'], partial_json_path, log_detail)
            result['json_updated'] = json_result['updated']
            result['errors'].extend(json_result['errors'])
            result['details'].extend(json_result.get('details', []))
            if not json_result['success']:
                result['success'] = False
        
    except Exception as e:
        error_msg = f"ç”Ÿæˆéƒ¨åˆ†æª”æ¡ˆå¤±æ•—ï¼š{e}"
        result['errors'].append(error_msg)
        result['success'] = False
        log_detail(f"éŒ¯èª¤: {error_msg}")
    
    return result


def generate_summary_report(results: dict, output_dir: Path, timestamp: str, log_detail):
    """ç”Ÿæˆè™•ç†æ‘˜è¦å ±å‘Š"""
    summary_file = output_dir / f"partial_processing_summary_{timestamp}.txt"
    
    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"æ•æ„Ÿè©ä¿®æ­£éƒ¨åˆ†æª”æ¡ˆè™•ç†æ‘˜è¦å ±å‘Š\n")
            f.write(f"ç”Ÿæˆæ™‚é–“ï¼š{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*50}\n\n")
            
            total_po_updates = 0
            total_json_updates = 0
            successful_business_types = []
            failed_business_types = []
            
            for bt_code, result in results.items():
                f.write(f"æ¥­æ…‹ï¼š{bt_code}\n")
                f.write(f"ç‹€æ…‹ï¼š{'æˆåŠŸ' if result['success'] else 'å¤±æ•—'}\n")
                f.write(f"PO æ›´æ–°æ•¸é‡ï¼š{result['po_updated']}\n")
                f.write(f"JSON æ›´æ–°æ•¸é‡ï¼š{result['json_updated']}\n")
                
                if result['success']:
                    successful_business_types.append(bt_code)
                    total_po_updates += result['po_updated']
                    total_json_updates += result['json_updated']
                else:
                    failed_business_types.append(bt_code)
                
                if result.get('errors'):
                    f.write(f"éŒ¯èª¤ï¼š\n")
                    for error in result['errors']:
                        f.write(f"  - {error}\n")
                
                if result.get('details'):
                    f.write(f"è©³ç´°æ›´æ–°è¨˜éŒ„ï¼š\n")
                    for detail in result['details'][:20]:  # é™åˆ¶é¡¯ç¤ºå‰20æ¢
                        f.write(f"  - {detail}\n")
                            
                    if len(result['details']) > 20:
                        f.write(f"  ... é‚„æœ‰ {len(result['details']) - 20} æ¢è¨˜éŒ„\n")
                
                f.write(f"\n{'-'*30}\n\n")
            
            # ç¸½è¨ˆçµ±è¨ˆ
            f.write(f"è™•ç†ç¸½çµï¼š\n")
            f.write(f"æˆåŠŸæ¥­æ…‹ï¼š{len(successful_business_types)}\n")
            f.write(f"å¤±æ•—æ¥­æ…‹ï¼š{len(failed_business_types)}\n")
            f.write(f"ç¸½ PO æ›´æ–°ï¼š{total_po_updates}\n")
            f.write(f"ç¸½ JSON æ›´æ–°ï¼š{total_json_updates}\n")
            f.write(f"ç¸½æ›´æ–°é …ç›®ï¼š{total_po_updates + total_json_updates}\n")
            
            if successful_business_types:
                f.write(f"\næˆåŠŸçš„æ¥­æ…‹ï¼š{', '.join(successful_business_types)}\n")
            
            if failed_business_types:
                f.write(f"å¤±æ•—çš„æ¥­æ…‹ï¼š{', '.join(failed_business_types)}\n")
            
            # éƒ¨åˆ†æª”æ¡ˆèªªæ˜
            f.write(f"\néƒ¨åˆ†æª”æ¡ˆèªªæ˜ï¼š\n")
            f.write(f"- æœ¬æ¬¡ç”Ÿæˆçš„æ˜¯éƒ¨åˆ†æª”æ¡ˆï¼ŒåªåŒ…å«æœ‰ä¿®æ”¹çš„ç¿»è­¯é …ç›®\n")
            f.write(f"- PO æª”æ¡ˆï¼šmessages_<æ¥­æ…‹>_partial.po\n")
            f.write(f"- JSON æª”æ¡ˆï¼š<èªè¨€>_<æ¥­æ…‹>_partial.json\n")
            f.write(f"- éƒ¨åˆ†æª”æ¡ˆä¿æŒèˆ‡åŸæª”æ¡ˆç›¸åŒçš„çµæ§‹å’Œå±¤ç´š\n")
            f.write(f"- å¯ä»¥ç›´æ¥ç”¨æ–¼ i18n ç³»çµ±æˆ–åˆä½µå›åŸæª”æ¡ˆ\n")
            
            f.write(f"\nä½¿ç”¨å»ºè­°ï¼š\n")
            f.write(f"- æª¢æŸ¥ç”Ÿæˆçš„éƒ¨åˆ†æª”æ¡ˆå…§å®¹æ˜¯å¦æ­£ç¢º\n")
            f.write(f"- ç¢ºèªç¿»è­¯é …ç›®çš„å±¤ç´šçµæ§‹æ­£ç¢º\n")
            f.write(f"- åœ¨ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨å‰é€²è¡Œæ¸¬è©¦\n")
            f.write(f"- è€ƒæ…®å°‡éƒ¨åˆ†æª”æ¡ˆåˆä½µå›ä¸»æª”æ¡ˆ\n")
        
        log_detail(f"æ‘˜è¦å ±å‘Šå·²ç”Ÿæˆï¼š{summary_file}")
        
    except Exception as e:
        log_detail(f"ç”Ÿæˆæ‘˜è¦å ±å‘Šå¤±æ•—ï¼š{e}")


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("ğŸš€ é–‹å§‹ç”Ÿæˆæ•æ„Ÿè©ä¿®æ­£éƒ¨åˆ†æª”æ¡ˆ (v1.0)")
    
    # è¼‰å…¥é…ç½®
    config = get_config()
    config.print_config_summary()
    
    # è™•ç†å‘½ä»¤åˆ—åƒæ•¸
    parser = argparse.ArgumentParser(description='ç”Ÿæˆæ•æ„Ÿè©ä¿®æ­£éƒ¨åˆ†æª”æ¡ˆ')
    parser.add_argument('--language', '-l', 
                       help='æŒ‡å®šè¦è™•ç†çš„èªè¨€ï¼ˆè‹¥æœªæŒ‡å®šå°‡è‡ªå‹•æª¢æ¸¬ï¼‰')
    parser.add_argument('--business-types', '-b',
                       nargs='+',
                       choices=list(config.get_business_types().keys()) + ['all'],
                       help='æŒ‡å®šè¦è™•ç†çš„æ¥­æ…‹ (å¯å¤šé¸ï¼Œæˆ–ä½¿ç”¨ all)')
    parser.add_argument('--list-files', action='store_true',
                       help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ tobemodified æª”æ¡ˆ')
    
    args = parser.parse_args()
    
    # æª¢æ¸¬å¯ç”¨çš„ tobemodified æª”æ¡ˆ
    available_files = detect_tobemodified_files(config)
    
    if args.list_files:
        print(f"\nğŸ“„ å¯ç”¨çš„ tobemodified æª”æ¡ˆï¼š")
        for lang, filepath in available_files.items():
            print(f"   {lang}: {filepath}")
        return
    
    if not available_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• tobemodified æª”æ¡ˆ")
        print("è«‹å…ˆåŸ·è¡Œ script_01_generate_xlsx.py ç”Ÿæˆæª”æ¡ˆ")
        sys.exit(1)
    
    # é¸æ“‡è¦è™•ç†çš„èªè¨€
    if args.language:
        if args.language not in available_files:
            print(f"âŒ èªè¨€ '{args.language}' çš„ tobemodified æª”æ¡ˆä¸å­˜åœ¨")
            print(f"å¯ç”¨èªè¨€ï¼š{list(available_files.keys())}")
            sys.exit(1)
        target_languages = [args.language]
        print(f"\nğŸŒ å°‡è™•ç†æŒ‡å®šèªè¨€ï¼š{args.language}")
    else:
        target_languages = list(available_files.keys())
        print(f"\nğŸŒ å°‡è™•ç†æ‰€æœ‰èªè¨€ï¼š{', '.join(target_languages)}")
    
    # é¸æ“‡æ¥­æ…‹
    target_business_types = choose_business_types(config, args)
    
    # è™•ç†æ¯å€‹èªè¨€
    success_count = 0
    total_count = len(target_languages)
    
    for language in target_languages:
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ è™•ç†èªè¨€ï¼š{language} (éƒ¨åˆ†æª”æ¡ˆæ¨¡å¼)")
        
        if process_language(config, language, target_business_types):
            success_count += 1
        else:
            print(f"âŒ {language} è™•ç†å¤±æ•—")
    
    # æœ€çµ‚å ±å‘Š
    print(f"\nğŸ‰ éƒ¨åˆ†æª”æ¡ˆç”Ÿæˆå®Œç•¢ï¼")
    print(f"ğŸ“Š æˆåŠŸè™•ç†ï¼š{success_count}/{total_count} å€‹èªè¨€")
    print(f"ğŸ’¡ ç”Ÿæˆçš„éƒ¨åˆ†æª”æ¡ˆç‰¹é»ï¼š")
    print(f"   - åªåŒ…å«æœ‰ä¿®æ”¹çš„ç¿»è­¯é …ç›®")
    print(f"   - ä¿æŒåŸæœ‰æª”æ¡ˆçµæ§‹å’Œå±¤ç´š")
    print(f"   - å¯ç›´æ¥ç”¨æ–¼ i18n ç³»çµ±")
    print(f"   - æª”æ¡ˆååŒ…å« '_partial' æ¨™è­˜")
    
    if success_count < total_count:
        sys.exit(1)


if __name__ == "__main__":
    main()